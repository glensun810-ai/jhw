#!/usr/bin/env python3
"""
è¯„åˆ†è®¡ç®—ä¿®å¤éªŒè¯è„šæœ¬

éªŒè¯å†…å®¹:
1. ä» geo_data æ„å»º judge_results
2. è°ƒç”¨ ScoringEngine è®¡ç®—åˆ†æ•°
3. éªŒè¯åˆ†æ•°èŒƒå›´åˆç†æ€§

æ‰§è¡Œï¼špython3 test_scoring_fix.py
"""

import sys
sys.path.insert(0, '/Users/sgl/PycharmProjects/PythonProject/backend_python')

from scoring_engine import ScoringEngine
from enhanced_scoring_engine import calculate_enhanced_scores
from ai_judge_module import JudgeResult, ConfidenceLevel


def test_geo_data_to_score():
    """æµ‹è¯•ä» geo_data åˆ°åˆ†æ•°çš„è½¬æ¢"""
    print("\n" + "="*60)
    print("æµ‹è¯• 1: geo_data åˆ°åˆ†æ•°çš„è½¬æ¢")
    print("="*60)
    
    # æ¨¡æ‹Ÿ geo_data
    test_cases = [
        {'rank': 1, 'sentiment': 0.8, 'expected_score_range': (85, 100)},
        {'rank': 2, 'sentiment': 0.5, 'expected_score_range': (80, 95)},
        {'rank': 3, 'sentiment': 0.3, 'expected_score_range': (75, 90)},
        {'rank': 5, 'sentiment': 0.2, 'expected_score_range': (60, 80)},
        {'rank': 8, 'sentiment': -0.3, 'expected_score_range': (40, 60)},
        {'rank': -1, 'sentiment': 0.5, 'expected_score_range': (30, 50)},  # æœªå…¥æ¦œ
    ]
    
    all_passed = True
    
    for i, case in enumerate(test_cases, 1):
        rank = case['rank']
        sentiment = case['sentiment']
        expected_range = case['expected_score_range']
        
        # è®¡ç®— accuracy_score (ä¸ nxm_execution_engine.py ä¸­é€»è¾‘ä¸€è‡´)
        if rank <= 0:
            accuracy_score = 30 + sentiment * 20
        elif rank <= 3:
            accuracy_score = 85 + (3 - rank) * 5 + sentiment * 10
        elif rank <= 6:
            accuracy_score = 65 + (6 - rank) * 5 + sentiment * 10
        else:
            accuracy_score = 45 + (10 - rank) * 3 + sentiment * 10
        
        accuracy_score = max(0, min(100, accuracy_score))
        
        # è®¡ç®— sentiment_score
        sentiment_score = (sentiment + 1) * 50
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é¢„æœŸèŒƒå›´å†…
        in_range = expected_range[0] <= accuracy_score <= expected_range[1]
        status = "âœ…" if in_range else "âŒ"
        
        print(f"\n  æµ‹è¯• {i}: rank={rank}, sentiment={sentiment}")
        print(f"    accuracy_score: {accuracy_score:.1f} (é¢„æœŸï¼š{expected_range[0]}-{expected_range[1]}) {status}")
        print(f"    sentiment_score: {sentiment_score:.1f}")
        
        if not in_range:
            all_passed = False
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯• 1 ç»“æœï¼š{'âœ… é€šè¿‡' if all_passed else 'âŒ å¤±è´¥'}")
    print(f"{'='*60}")
    return all_passed


def test_scoring_engine():
    """æµ‹è¯•è¯„åˆ†å¼•æ“"""
    print("\n" + "="*60)
    print("æµ‹è¯• 2: ScoringEngine è¯„åˆ†å¼•æ“")
    print("="*60)
    
    # æ„å»º judge_results
    judge_results = []
    
    # æ¨¡æ‹Ÿ 3 ä¸ª AI å¹³å°çš„è¯„åˆ¤ç»“æœ
    test_data = [
        {'rank': 2, 'sentiment': 0.6},
        {'rank': 3, 'sentiment': 0.4},
        {'rank': 1, 'sentiment': 0.8},
    ]
    
    for data in test_data:
        rank = data['rank']
        sentiment = data['sentiment']
        
        # è®¡ç®—å„ç»´åº¦åˆ†æ•°
        if rank <= 0:
            accuracy_score = 30 + sentiment * 20
        elif rank <= 3:
            accuracy_score = 85 + (3 - rank) * 5 + sentiment * 10
        elif rank <= 6:
            accuracy_score = 65 + (6 - rank) * 5 + sentiment * 10
        else:
            accuracy_score = 45 + (10 - rank) * 3 + sentiment * 10
        
        accuracy_score = max(0, min(100, accuracy_score))
        completeness_score = 70
        sentiment_score = (sentiment + 1) * 50
        
        judge_result = JudgeResult(
            accuracy_score=accuracy_score,
            completeness_score=completeness_score,
            sentiment_score=sentiment_score,
            purity_score=sentiment_score * 0.9,
            consistency_score=accuracy_score * 0.95,
            judgement=f"Rank: {rank}, Sentiment: {sentiment:.2f}",
            confidence_level=ConfidenceLevel.HIGH if rank > 0 else ConfidenceLevel.MEDIUM
        )
        judge_results.append(judge_result)
    
    # è°ƒç”¨è¯„åˆ†å¼•æ“
    scoring_engine = ScoringEngine()
    result = scoring_engine.calculate(judge_results)
    
    print(f"\n  è¾“å…¥ï¼š{len(judge_results)} ä¸ª judge_results")
    print(f"  è¾“å‡º:")
    print(f"    GEO åˆ†æ•°ï¼š{result.geo_score}")
    print(f"    ç­‰çº§ï¼š{result.grade}")
    print(f"    æ ‡ç­¾ï¼š{result.label}")
    print(f"    æƒå¨åº¦ï¼š{result.authority_score:.1f}")
    print(f"    å¯è§åº¦ï¼š{result.visibility_score:.1f}")
    print(f"    æƒ…æ„Ÿåº¦ï¼š{result.sentiment_score:.1f}")
    print(f"    çº¯å‡€åº¦ï¼š{result.purity_score:.1f}")
    print(f"    ä¸€è‡´æ€§ï¼š{result.consistency_score:.1f}")
    
    # éªŒè¯åˆ†æ•°åˆç†æ€§
    valid_score = 0 <= result.geo_score <= 100
    valid_grade = result.grade in ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D', 'D-']
    
    print(f"\n  éªŒè¯:")
    print(f"    åˆ†æ•°èŒƒå›´ï¼š{'âœ…' if valid_score else 'âŒ'} (0-100)")
    print(f"    ç­‰çº§æœ‰æ•ˆï¼š{'âœ…' if valid_grade else 'âŒ'}")
    
    passed = valid_score and valid_grade
    print(f"\n{'='*60}")
    print(f"æµ‹è¯• 2 ç»“æœï¼š{'âœ… é€šè¿‡' if passed else 'âŒ å¤±è´¥'}")
    print(f"{'='*60}")
    return passed


def test_enhanced_scoring():
    """æµ‹è¯•å¢å¼ºè¯„åˆ†å¼•æ“"""
    print("\n" + "="*60)
    print("æµ‹è¯• 3: EnhancedScoringEngine å¢å¼ºè¯„åˆ†")
    print("="*60)
    
    judge_results = [
        JudgeResult(
            accuracy_score=85,
            completeness_score=78,
            sentiment_score=82,
            purity_score=75,
            consistency_score=80,
            judgement='æµ‹è¯•',
            confidence_level=ConfidenceLevel.HIGH
        )
    ]
    
    result = calculate_enhanced_scores(judge_results, brand_name='åä¸º')
    
    print(f"\n  å“ç‰Œï¼šåä¸º")
    print(f"  GEO åˆ†æ•°ï¼š{result.geo_score}")
    print(f"  ç­‰çº§ï¼š{result.grade} ({result.label})")
    print(f"  è®¤çŸ¥ç½®ä¿¡åº¦ï¼š{result.cognitive_confidence:.2f}")
    print(f"  å»ºè®®æ•°é‡ï¼š{len(result.recommendations)}")
    print(f"  æ€»ç»“ï¼š{result.summary[:50]}...")
    
    passed = result.geo_score > 0 and result.grade is not None
    print(f"\n{'='*60}")
    print(f"æµ‹è¯• 3 ç»“æœï¼š{'âœ… é€šè¿‡' if passed else 'âŒ å¤±è´¥'}")
    print(f"{'='*60}")
    return passed


def main():
    print("\n" + "="*60)
    print("  è¯„åˆ†è®¡ç®—ä¿®å¤éªŒè¯æµ‹è¯•")
    print("  Scoring Fix Verification Test")
    print("="*60)
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results.append(("geo_data è½¬æ¢", test_geo_data_to_score()))
    results.append(("è¯„åˆ†å¼•æ“", test_scoring_engine()))
    results.append(("å¢å¼ºè¯„åˆ†", test_enhanced_scoring()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*60)
    print("  æµ‹è¯•æ±‡æ€»")
    print("="*60)
    
    for name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {name}: {status}")
    
    total_passed = sum(1 for _, p in results if p)
    total = len(results)
    
    print(f"\n  æ€»è®¡ï¼š{total_passed}/{total} æµ‹è¯•é€šè¿‡")
    print(f"{'='*60}")
    
    if total_passed == total:
        print("\n  ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è¯„åˆ†è®¡ç®—åŠŸèƒ½æ­£å¸¸ã€‚")
        return True
    else:
        print("\n  âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ã€‚")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
