"""
æ•°å­—èµ„äº§ä¿æŠ¤æ¨¡å—

æ ¸å¿ƒåŸåˆ™ï¼š
1. AI å“åº”è¿”å›å 1 ç§’å†…æŒä¹…åŒ–
2. è‡³å°‘ä¸¤ä»½æ•°æ®å‰¯æœ¬
3. äº‹åŠ¡ä¿æŠ¤ç¡®ä¿ä¸€è‡´æ€§
4. å®šæœŸå¤‡ä»½ç¡®ä¿å¯æ¢å¤

å­˜å‚¨å±‚çº§ï¼š
1. å†…å­˜å±‚ - execution_storeï¼ˆå®æ—¶è®¿é—®ï¼‰
2. æ•°æ®åº“å±‚ - SQLiteï¼ˆä¸»å­˜å‚¨ï¼‰
3. æ–‡ä»¶å±‚ - JSON æ—¥å¿—ï¼ˆå®¡è®¡è¿½è¸ªï¼‰
4. å¤‡ä»½å±‚ - å®šæ—¶å¤‡ä»½ï¼ˆç¾éš¾æ¢å¤ï¼‰
"""

import json
import os
import shutil
import hashlib
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from contextlib import contextmanager
from wechat_backend.logging_config import db_logger, api_logger
from wechat_backend.database_connection_pool import get_db_pool


# ==================== é…ç½® ====================

# å¤‡ä»½ç›®å½•
BACKUP_DIR = os.path.join(os.path.dirname(__file__), '..', '..', 'data_backups')
os.makedirs(BACKUP_DIR, exist_ok=True)

# å®¡è®¡æ—¥å¿—ç›®å½•
AUDIT_LOG_DIR = os.path.join(os.path.dirname(__file__), '..', '..', 'audit_logs')
os.makedirs(AUDIT_LOG_DIR, exist_ok=True)

# ä¿ç•™å¤©æ•°
BACKUP_RETENTION_DAYS = 30
AUDIT_LOG_RETENTION_DAYS = 90


# ==================== æ ¸å¿ƒå‡½æ•° ====================

@contextmanager
def transaction_context():
    """äº‹åŠ¡ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    conn = get_db_pool().get_connection()
    try:
        conn.execute('BEGIN IMMEDIATE')  # ç«‹å³è·å–å†™é”
        yield conn
        conn.execute('COMMIT')
    except Exception as e:
        conn.execute('ROLLBACK')
        db_logger.error(f"äº‹åŠ¡å¤±è´¥ï¼š{e}")
        raise
    finally:
        get_db_pool().return_connection(conn)


def calculate_checksum(data: Dict[str, Any]) -> str:
    """è®¡ç®—æ•°æ®æ ¡éªŒå’Œ"""
    # æ’åºé”®ä»¥ç¡®ä¿ä¸€è‡´æ€§
    json_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(json_str.encode('utf-8')).hexdigest()[:16]


def save_diagnosis_result_to_db(
    execution_id: str,
    user_id: str,
    brand_name: str,
    results: List[Dict[str, Any]],
    metadata: Optional[Dict[str, Any]] = None
) -> int:
    """
    ä¿å­˜è¯Šæ–­ç»“æœåˆ°æ•°æ®åº“ï¼ˆå¸¦äº‹åŠ¡ä¿æŠ¤ï¼‰
    
    å‚æ•°:
    - execution_id: æ‰§è¡Œ ID
    - user_id: ç”¨æˆ· ID
    - brand_name: å“ç‰Œåç§°
    - results: ç»“æœåˆ—è¡¨
    - metadata: å…ƒæ•°æ®
    
    è¿”å›:
    - è®°å½• ID
    """
    try:
        # è®¡ç®—æ ¡éªŒå’Œ
        checksum = calculate_checksum({
            'execution_id': execution_id,
            'results': results
        })
        
        # åºåˆ—åŒ–æ•°æ®
        results_json = json.dumps(results, ensure_ascii=False)
        metadata_json = json.dumps(metadata or {}, ensure_ascii=False)
        
        with transaction_context() as conn:
            cursor = conn.cursor()
            
            # 1. ä¿å­˜åˆ°ä¸»è¡¨
            cursor.execute('''
                INSERT OR REPLACE INTO diagnosis_results (
                    execution_id, user_id, brand_name, results,
                    metadata, checksum, created_at, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                execution_id,
                user_id,
                brand_name,
                results_json,
                metadata_json,
                checksum,
                datetime.now().isoformat(),
                datetime.now().isoformat()
            ))
            
            record_id = cursor.lastrowid
            
            # 2. ä¿å­˜åˆ°å¤‡ä»½è¡¨ï¼ˆåŒé‡ä¿æŠ¤ï¼‰
            cursor.execute('''
                INSERT INTO diagnosis_results_backup (
                    execution_id, user_id, brand_name, results,
                    checksum, backup_at
                ) VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                execution_id,
                user_id,
                brand_name,
                results_json,
                checksum,
                datetime.now().isoformat()
            ))
            
            db_logger.info(f"âœ… è¯Šæ–­ç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“ï¼š{execution_id}, è®°å½• ID: {record_id}")
            return record_id
            
    except Exception as e:
        db_logger.error(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥ï¼š{execution_id}, é”™è¯¯ï¼š{e}")
        # é™çº§ï¼šä¿å­˜åˆ°æ–‡ä»¶
        save_to_emergency_log(execution_id, results, metadata)
        raise


def save_to_emergency_log(
    execution_id: str,
    results: List[Dict[str, Any]],
    metadata: Optional[Dict[str, Any]] = None
) -> str:
    """
    ç´§æ€¥æ—¥å¿—ä¿å­˜ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
    
    å½“æ•°æ®åº“ä¸å¯ç”¨æ—¶ï¼Œä¿å­˜åˆ°æ–‡ä»¶
    """
    try:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"emergency_{execution_id}_{timestamp}.json"
        filepath = os.path.join(AUDIT_LOG_DIR, filename)
        
        data = {
            'execution_id': execution_id,
            'timestamp': datetime.now().isoformat(),
            'results': results,
            'metadata': metadata or {},
            'checksum': calculate_checksum({'execution_id': execution_id, 'results': results})
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        api_logger.warning(f"âš ï¸ ç´§æ€¥æ—¥å¿—å·²ä¿å­˜ï¼š{filepath}")
        return filepath
        
    except Exception as e:
        api_logger.error(f"âŒ ç´§æ€¥æ—¥å¿—ä¿å­˜å¤±è´¥ï¼š{e}")
        return None


def get_diagnosis_result_by_execution_id(execution_id: str) -> Optional[Dict[str, Any]]:
    """
    æ ¹æ®æ‰§è¡Œ ID è·å–è¯Šæ–­ç»“æœ
    
    ä¼˜å…ˆçº§ï¼š
    1. ä¸»æ•°æ®åº“
    2. å¤‡ä»½è¡¨
    3. ç´§æ€¥æ—¥å¿—æ–‡ä»¶
    """
    try:
        conn = get_db_pool().get_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # 1. å°è¯•ä¸»è¡¨
        cursor.execute('''
            SELECT * FROM diagnosis_results
            WHERE execution_id = ?
            ORDER BY updated_at DESC
            LIMIT 1
        ''', (execution_id,))
        
        row = cursor.fetchone()
        get_db_pool().return_connection(conn)
        
        if row:
            result = dict(row)
            result['results'] = json.loads(result['results'])
            result['metadata'] = json.loads(result['metadata'])
            db_logger.info(f"âœ… ä»ä¸»è¡¨è·å–ç»“æœï¼š{execution_id}")
            return result
        
        # 2. å°è¯•å¤‡ä»½è¡¨
        cursor = conn.cursor()
        cursor.execute('''
            SELECT * FROM diagnosis_results_backup
            WHERE execution_id = ?
            ORDER BY backup_at DESC
            LIMIT 1
        ''', (execution_id,))
        
        row = cursor.fetchone()
        get_db_pool().return_connection(conn)
        
        if row:
            result = dict(row)
            result['results'] = json.loads(result['results'])
            db_logger.info(f"âœ… ä»å¤‡ä»½è¡¨è·å–ç»“æœï¼š{execution_id}")
            return result
        
        # 3. å°è¯•ç´§æ€¥æ—¥å¿—
        for filename in os.listdir(AUDIT_LOG_DIR):
            if execution_id in filename and filename.endswith('.json'):
                filepath = os.path.join(AUDIT_LOG_DIR, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        db_logger.info(f"âœ… ä»ç´§æ€¥æ—¥å¿—è·å–ç»“æœï¼š{execution_id}")
                        return data
                except Exception:
                    continue
        
        db_logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç»“æœï¼š{execution_id}")
        return None
        
    except Exception as e:
        db_logger.error(f"âŒ è·å–ç»“æœå¤±è´¥ï¼š{execution_id}, é”™è¯¯ï¼š{e}")
        return None


def verify_data_integrity(execution_id: str, results: List[Dict[str, Any]]) -> bool:
    """
    éªŒè¯æ•°æ®å®Œæ•´æ€§
    
    é€šè¿‡æ ¡éªŒå’ŒéªŒè¯æ•°æ®æ˜¯å¦è¢«ç¯¡æ”¹
    """
    stored_result = get_diagnosis_result_by_execution_id(execution_id)
    
    if not stored_result:
        return False
    
    stored_checksum = stored_result.get('checksum', '')
    current_checksum = calculate_checksum({
        'execution_id': execution_id,
        'results': results
    })
    
    is_valid = stored_checksum == current_checksum
    
    if not is_valid:
        db_logger.error(f"âŒ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥ï¼š{execution_id}")
        db_logger.error(f"   å­˜å‚¨æ ¡éªŒå’Œï¼š{stored_checksum}")
        db_logger.error(f"   å½“å‰æ ¡éªŒå’Œï¼š{current_checksum}")
    
    return is_valid


# ==================== å¤‡ä»½æœºåˆ¶ ====================

def create_daily_backup() -> Dict[str, Any]:
    """
    åˆ›å»ºæ¯æ—¥å¤‡ä»½
    
    è¿”å›:
    - å¤‡ä»½ç»Ÿè®¡ä¿¡æ¯
    """
    timestamp = datetime.now().strftime('%Y%m%d')
    backup_stats = {
        'timestamp': timestamp,
        'database_backup': None,
        'json_export': None,
        'records_count': 0,
        'size_bytes': 0
    }
    
    try:
        # 1. SQLite æ•°æ®åº“å¤‡ä»½
        db_path = os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'diagnosis.db')
        if os.path.exists(db_path):
            backup_db_path = os.path.join(BACKUP_DIR, f'db_{timestamp}.db')
            shutil.copy2(db_path, backup_db_path)
            backup_stats['database_backup'] = backup_db_path
            db_logger.info(f"âœ… æ•°æ®åº“å¤‡ä»½å®Œæˆï¼š{backup_db_path}")
        
        # 2. å¯¼å‡ºæ‰€æœ‰ç»“æœä¸º JSON
        conn = get_db_pool().get_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM diagnosis_results ORDER BY created_at DESC')
        rows = cursor.fetchall()
        get_db_pool().return_connection(conn)
        
        results = []
        for row in rows:
            item = dict(row)
            item['results'] = json.loads(item['results'])
            item['metadata'] = json.loads(item['metadata'])
            results.append(item)
        
        backup_stats['records_count'] = len(results)
        
        # ä¿å­˜ JSON å¯¼å‡º
        json_path = os.path.join(BACKUP_DIR, f'results_{timestamp}.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        backup_stats['json_export'] = json_path
        backup_stats['size_bytes'] = os.path.getsize(json_path)
        
        db_logger.info(f"âœ… JSON å¯¼å‡ºå®Œæˆï¼š{json_path}, è®°å½•æ•°ï¼š{len(results)}")
        
    except Exception as e:
        db_logger.error(f"âŒ å¤‡ä»½å¤±è´¥ï¼š{e}")
        backup_stats['error'] = str(e)
    
    return backup_stats


def cleanup_old_backups(days: int = BACKUP_RETENTION_DAYS) -> Dict[str, Any]:
    """
    æ¸…ç†æ—§å¤‡ä»½
    
    ä¿ç•™æœ€è¿‘ N å¤©çš„å¤‡ä»½
    """
    cleanup_stats = {
        'deleted_files': 0,
        'freed_bytes': 0,
        'errors': []
    }
    
    try:
        cutoff_date = datetime.now() - timedelta(days=days)
        cutoff_filename = cutoff_date.strftime('%Y%m%d')
        
        for filename in os.listdir(BACKUP_DIR):
            # ä»æ–‡ä»¶åæå–æ—¥æœŸ
            if filename.startswith('db_') or filename.startswith('results_'):
                file_date = filename.split('_')[1].split('.')[0]
                if file_date < cutoff_filename:
                    filepath = os.path.join(BACKUP_DIR, filename)
                    try:
                        file_size = os.path.getsize(filepath)
                        os.remove(filepath)
                        cleanup_stats['deleted_files'] += 1
                        cleanup_stats['freed_bytes'] += file_size
                        db_logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§å¤‡ä»½ï¼š{filename}")
                    except Exception as e:
                        cleanup_stats['errors'].append(f"{filename}: {e}")
                        db_logger.error(f"âŒ åˆ é™¤å¤‡ä»½å¤±è´¥ï¼š{filename}, é”™è¯¯ï¼š{e}")
        
        db_logger.info(f"âœ… æ¸…ç†å®Œæˆï¼šåˆ é™¤ {cleanup_stats['deleted_files']} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {cleanup_stats['freed_bytes']} å­—èŠ‚")
        
    except Exception as e:
        db_logger.error(f"âŒ æ¸…ç†å¤±è´¥ï¼š{e}")
        cleanup_stats['errors'].append(f"cleanup: {e}")
    
    return cleanup_stats


# ==================== åˆå§‹åŒ– ====================

def init_database_tables():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
    with transaction_context() as conn:
        cursor = conn.cursor()
        
        # ä¸»è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS diagnosis_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                execution_id TEXT UNIQUE NOT NULL,
                user_id TEXT NOT NULL,
                brand_name TEXT NOT NULL,
                results TEXT NOT NULL,
                metadata TEXT,
                checksum TEXT NOT NULL,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL
            )
        ''')
        
        # å¤‡ä»½è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS diagnosis_results_backup (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                execution_id TEXT NOT NULL,
                user_id TEXT NOT NULL,
                brand_name TEXT NOT NULL,
                results TEXT NOT NULL,
                checksum TEXT NOT NULL,
                backup_at TEXT NOT NULL
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_execution_id
            ON diagnosis_results(execution_id)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_user_id
            ON diagnosis_results(user_id)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_backup_execution_id
            ON diagnosis_results_backup(execution_id)
        ''')
        
        db_logger.info("âœ… æ•°æ®åº“è¡¨åˆå§‹åŒ–å®Œæˆ")


# æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–
try:
    init_database_tables()
except Exception as e:
    db_logger.error(f"âš ï¸ æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
