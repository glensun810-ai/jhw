"""
å“ç‰Œè¯Šæ–­æŠ¥å‘Šå­˜å‚¨æ¶æ„ - Repository å±‚å®ç°

æ ¸å¿ƒåŸåˆ™ï¼š
1. æ•°æ®åº“æ˜¯å”¯ä¸€äº‹å®æº
2. å†å²æ•°æ®ä¸å¯å˜
3. ç‰ˆæœ¬æ§åˆ¶
4. åˆ†å±‚å­˜å‚¨
5. å®Œæ•´æ€§æ ¡éªŒ

ä½œè€…ï¼šé¦–å¸­å…¨æ ˆå·¥ç¨‹å¸ˆ
æ—¥æœŸï¼š2026-02-26
ç‰ˆæœ¬ï¼š1.0
"""

import json
import gzip
import hashlib
import os
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from contextlib import contextmanager
from wechat_backend.logging_config import db_logger, api_logger
from wechat_backend.database_connection_pool import get_db_pool


# ==================== é…ç½® ====================

# å­˜å‚¨è·¯å¾„
DATA_DIR = os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'diagnosis')
REPORTS_DIR = os.path.join(DATA_DIR, 'reports')
ARCHIVES_DIR = os.path.join(DATA_DIR, 'archives')
BACKUPS_DIR = os.path.join(DATA_DIR, 'backups')

# åˆ›å»ºç›®å½•
for dir_path in [REPORTS_DIR, ARCHIVES_DIR, BACKUPS_DIR]:
    os.makedirs(dir_path, exist_ok=True)
    # åˆ›å»ºå¹´æœˆæ—¥ç›®å½•ç»“æ„
    now = datetime.now()
    for year in range(now.year - 1, now.year + 1):
        for month in range(1, 13):
            year_month_dir = os.path.join(REPORTS_DIR, str(year), f'{month:02d}')
            os.makedirs(year_month_dir, exist_ok=True)

# æ•°æ® schema ç‰ˆæœ¬
DATA_SCHEMA_VERSION = '1.0'


# ==================== å·¥å…·å‡½æ•° ====================

def calculate_checksum(data: Dict[str, Any]) -> str:
    """è®¡ç®—æ•°æ® SHA256 æ ¡éªŒå’Œ"""
    json_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(json_str.encode('utf-8')).hexdigest()


def verify_checksum(data: Dict[str, Any], checksum: str) -> bool:
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    return calculate_checksum(data) == checksum


def get_server_version() -> str:
    """è·å–æœåŠ¡å™¨ç‰ˆæœ¬å·"""
    return os.getenv('SERVER_VERSION', '2.0.0')


def get_file_archive_path(execution_id: str, created_at: datetime) -> str:
    """è·å–æ–‡ä»¶å½’æ¡£è·¯å¾„"""
    year = created_at.year
    month = created_at.month
    day = created_at.day
    
    return os.path.join(
        REPORTS_DIR,
        str(year),
        f'{month:02d}',
        f'{day:02d}',
        f'{execution_id}.json'
    )


# ==================== Repository å±‚ ====================

class DiagnosisReportRepository:
    """
    è¯Šæ–­æŠ¥å‘Šä»“åº“ - æ•°æ®è®¿é—®å±‚
    
    èŒè´£ï¼š
    1. æ•°æ®åº“ CRUD æ“ä½œ
    2. äº‹åŠ¡ç®¡ç†
    3. æ•°æ®éªŒè¯
    """
    
    @contextmanager
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        conn = get_db_pool().get_connection()
        try:
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            db_logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥ï¼š{e}")
            raise
        finally:
            get_db_pool().return_connection(conn)
    
    def create(self, execution_id: str, user_id: str, config: Dict[str, Any]) -> int:
        """
        åˆ›å»ºè¯Šæ–­æŠ¥å‘Š
        
        å‚æ•°:
            execution_id: æ‰§è¡Œ ID
            user_id: ç”¨æˆ· ID
            config: è¯Šæ–­é…ç½® {brand_name, competitor_brands, selected_models, custom_questions}
        
        è¿”å›:
            report_id: æŠ¥å‘Š ID
        """
        now = datetime.now().isoformat()
        checksum = calculate_checksum({
            'execution_id': execution_id,
            'user_id': user_id,
            'config': config
        })
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO diagnosis_reports (
                    execution_id, user_id,
                    brand_name, competitor_brands, selected_models, custom_questions,
                    status, progress, stage, is_completed,
                    created_at, updated_at,
                    data_schema_version, server_version,
                    checksum
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                execution_id,
                user_id,
                config.get('brand_name', ''),
                json.dumps(config.get('competitor_brands', [])),
                json.dumps(config.get('selected_models', [])),
                json.dumps(config.get('custom_questions', [])),
                'processing',
                0,
                'init',
                0,
                now,
                now,
                DATA_SCHEMA_VERSION,
                get_server_version(),
                checksum
            ))
            
            report_id = cursor.lastrowid
            db_logger.info(f"âœ… åˆ›å»ºè¯Šæ–­æŠ¥å‘Šï¼š{execution_id}, report_id: {report_id}")
            return report_id
    
    def update_status(self, execution_id: str, status: str, progress: int,
                     stage: str, is_completed: bool = False) -> bool:
        """
        æ›´æ–°æŠ¥å‘ŠçŠ¶æ€ï¼ˆP0 ä¿®å¤ï¼šç¡®ä¿ status å’Œ stage åŒæ­¥ï¼‰
        
        Args:
            execution_id: æ‰§è¡Œ ID
            status: çŠ¶æ€ï¼ˆinitializing/ai_fetching/analyzing/completed/failedï¼‰
            progress: è¿›åº¦ï¼ˆ0-100ï¼‰
            stage: é˜¶æ®µï¼ˆä¸ status ä¿æŒä¸€è‡´ï¼‰
            is_completed: æ˜¯å¦å®Œæˆ
        """
        now = datetime.now().isoformat()

        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                UPDATE diagnosis_reports
                SET status = ?, progress = ?, stage = ?, is_completed = ?, updated_at = ?,
                    completed_at = CASE WHEN ? = 1 THEN ? ELSE completed_at END
                WHERE execution_id = ?
            ''', (
                status, progress, stage, 1 if is_completed else 0, now,
                1 if is_completed else 0, now,
                execution_id
            ))

            db_logger.info(f"ğŸ“Š æ›´æ–°è¯Šæ–­æŠ¥å‘ŠçŠ¶æ€ï¼š{execution_id}, status={status}, stage={stage}, progress={progress}")
            return cursor.rowcount > 0
    
    def update_status_sync(self, execution_id: str, status: str, progress: int = None,
                          is_completed: bool = False) -> bool:
        """
        P0 ä¿®å¤ï¼šç»Ÿä¸€çŠ¶æ€æ›´æ–°å‡½æ•°ï¼ˆç¡®ä¿ status å’Œ stage åŒæ­¥ï¼‰
        
        è‡ªåŠ¨æ ¹æ® status æ¨å¯¼ stageï¼Œé¿å…çŠ¶æ€ä¸ä¸€è‡´
        
        Args:
            execution_id: æ‰§è¡Œ ID
            status: çŠ¶æ€ï¼ˆinitializing/ai_fetching/analyzing/completed/failedï¼‰
            progress: è¿›åº¦ï¼ˆå¯é€‰ï¼Œé»˜è®¤æ ¹æ® status æ¨å¯¼ï¼‰
            is_completed: æ˜¯å¦å®Œæˆ
        """
        # çŠ¶æ€æ˜ å°„è¡¨
        status_stage_map = {
            'initializing': 'init',
            'ai_fetching': 'ai_fetching',
            'analyzing': 'analyzing',
            'completed': 'completed',
            'failed': 'failed',
            'partial_completed': 'completed'  # éƒ¨åˆ†å®Œæˆä¹Ÿè§†ä¸ºå®Œæˆ
        }
        
        # è‡ªåŠ¨æ¨å¯¼ stage
        stage = status_stage_map.get(status, status)
        
        # è‡ªåŠ¨æ¨å¯¼ progress
        if progress is None:
            progress_map = {
                'initializing': 0,
                'ai_fetching': 50,
                'analyzing': 80,
                'completed': 100,
                'failed': 0
            }
            progress = progress_map.get(status, 0)
        
        # è°ƒç”¨åŸæœ‰æ›´æ–°å‡½æ•°
        return self.update_status(execution_id, status, progress, stage, is_completed)
    
    def get_by_execution_id(self, execution_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®æ‰§è¡Œ ID è·å–æŠ¥å‘Š"""
        with self.get_connection() as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute('SELECT * FROM diagnosis_reports WHERE execution_id = ?', (execution_id,))

            row = cursor.fetchone()
            if row:
                result = dict(row)
                # è§£æ JSON å­—æ®µ
                result['competitor_brands'] = json.loads(result['competitor_brands'])
                result['selected_models'] = json.loads(result['selected_models'])
                result['custom_questions'] = json.loads(result['custom_questions'])
                return result
            return None
    
    def delete_by_execution_id(self, execution_id: str) -> bool:
        """
        P0 ä¿®å¤ï¼šæ ¹æ®æ‰§è¡Œ ID åˆ é™¤æŠ¥å‘Šï¼ˆç”¨äºæ¸…ç†ç©ºæŠ¥å‘Šï¼‰
        
        Args:
            execution_id: æ‰§è¡Œ ID
        
        Returns:
            bool: æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('DELETE FROM diagnosis_reports WHERE execution_id = ?', (execution_id,))
            
            deleted_count = cursor.rowcount
            if deleted_count > 0:
                db_logger.info(f"ğŸ—‘ï¸ åˆ é™¤è¯Šæ–­æŠ¥å‘Šï¼š{execution_id}")
            return deleted_count > 0

    def get_user_history(self, user_id: str, limit: int = 20, offset: int = 0) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·å†å²æŠ¥å‘Š"""
        with self.get_connection() as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute('''
                SELECT id, execution_id, brand_name, status, progress, stage, 
                       is_completed, created_at, completed_at
                FROM diagnosis_reports
                WHERE user_id = ?
                ORDER BY created_at DESC
                LIMIT ? OFFSET ?
            ''', (user_id, limit, offset))
            
            results = []
            for row in cursor.fetchall():
                item = dict(row)
                results.append(item)
            
            return results
    
    def create_snapshot(self, report_id: int, execution_id: str, 
                       snapshot_data: Dict[str, Any], reason: str) -> int:
        """åˆ›å»ºæŠ¥å‘Šå¿«ç…§"""
        now = datetime.now().isoformat()
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO diagnosis_snapshots (
                    report_id, execution_id,
                    snapshot_data, snapshot_reason, snapshot_version,
                    created_at
                ) VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                report_id,
                execution_id,
                json.dumps(snapshot_data, ensure_ascii=False),
                reason,
                DATA_SCHEMA_VERSION,
                now
            ))
            
            snapshot_id = cursor.lastrowid
            db_logger.info(f"âœ… åˆ›å»ºå¿«ç…§ï¼š{execution_id}, snapshot_id: {snapshot_id}")
            return snapshot_id


class DiagnosisResultRepository:
    """
    è¯Šæ–­ç»“æœä»“åº“ - æ•°æ®è®¿é—®å±‚
    
    èŒè´£ï¼š
    1. ç»“æœæ˜ç»† CRUD æ“ä½œ
    2. æ‰¹é‡æ“ä½œ
    3. æ•°æ®éªŒè¯
    """
    
    @contextmanager
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        conn = get_db_pool().get_connection()
        try:
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            db_logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥ï¼š{e}")
            raise
        finally:
            get_db_pool().return_connection(conn)
    
    def add(self, report_id: int, execution_id: str, result: Dict[str, Any]) -> int:
        """æ·»åŠ å•ä¸ªè¯Šæ–­ç»“æœ"""
        now = datetime.now().isoformat()
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO diagnosis_results (
                    report_id, execution_id,
                    brand, question, model,
                    response_content, response_latency,
                    geo_data,
                    quality_score, quality_level, quality_details,
                    status, error_message,
                    created_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                report_id,
                execution_id,
                result.get('brand', ''),
                result.get('question', ''),
                result.get('model', ''),
                result.get('response', {}).get('content', '') if isinstance(result.get('response'), dict) else '',
                result.get('response', {}).get('latency') if isinstance(result.get('response'), dict) else None,
                json.dumps(result.get('geo_data', {}), ensure_ascii=False),
                result.get('quality_score', 0),
                result.get('quality_level', 'unknown'),
                json.dumps(result.get('quality_details', {}), ensure_ascii=False),
                result.get('status', 'success'),
                result.get('error'),
                now
            ))
            
            result_id = cursor.lastrowid
            return result_id
    
    def add_batch(self, report_id: int, execution_id: str, 
                 results: List[Dict[str, Any]]) -> List[int]:
        """æ‰¹é‡æ·»åŠ è¯Šæ–­ç»“æœ"""
        result_ids = []
        for result in results:
            result_id = self.add(report_id, execution_id, result)
            result_ids.append(result_id)
        return result_ids
    
    def get_by_execution_id(self, execution_id: str) -> List[Dict[str, Any]]:
        """æ ¹æ®æ‰§è¡Œ ID è·å–æ‰€æœ‰ç»“æœ"""
        with self.get_connection() as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute('''
                SELECT * FROM diagnosis_results
                WHERE execution_id = ?
                ORDER BY brand, question, model
            ''', (execution_id,))
            
            results = []
            for row in cursor.fetchall():
                item = dict(row)
                # è§£æ JSON å­—æ®µ
                item['geo_data'] = json.loads(item['geo_data'])
                item['quality_details'] = json.loads(item['quality_details'])
                # æ„å»º response å¯¹è±¡
                item['response'] = {
                    'content': item['response_content'],
                    'latency': item['response_latency']
                }
                results.append(item)
            
            return results
    
    def get_by_report_id(self, report_id: int) -> List[Dict[str, Any]]:
        """æ ¹æ®æŠ¥å‘Š ID è·å–æ‰€æœ‰ç»“æœ"""
        with self.get_connection() as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute('''
                SELECT * FROM diagnosis_results
                WHERE report_id = ?
                ORDER BY brand, question, model
            ''', (report_id,))
            
            results = []
            for row in cursor.fetchall():
                item = dict(row)
                item['geo_data'] = json.loads(item['geo_data'])
                item['quality_details'] = json.loads(item['quality_details'])
                item['response'] = {
                    'content': item['response_content'],
                    'latency': item['response_latency']
                }
                results.append(item)
            
            return results


class DiagnosisAnalysisRepository:
    """
    è¯Šæ–­åˆ†æä»“åº“ - æ•°æ®è®¿é—®å±‚
    
    èŒè´£ï¼š
    1. é«˜çº§åˆ†ææ•°æ® CRUD æ“ä½œ
    2. æŒ‰ç±»å‹ç®¡ç†åˆ†ææ•°æ®
    """
    
    @contextmanager
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        conn = get_db_pool().get_connection()
        try:
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            db_logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥ï¼š{e}")
            raise
        finally:
            get_db_pool().return_connection(conn)
    
    def add(self, report_id: int, execution_id: str, 
            analysis_type: str, analysis_data: Dict[str, Any]) -> int:
        """æ·»åŠ åˆ†ææ•°æ®"""
        now = datetime.now().isoformat()
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO diagnosis_analysis (
                    report_id, execution_id,
                    analysis_type, analysis_data, analysis_version,
                    created_at
                ) VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                report_id,
                execution_id,
                analysis_type,
                json.dumps(analysis_data, ensure_ascii=False),
                DATA_SCHEMA_VERSION,
                now
            ))
            
            analysis_id = cursor.lastrowid
            return analysis_id
    
    def add_batch(self, report_id: int, execution_id: str, 
                 analyses: Dict[str, Dict[str, Any]]) -> List[int]:
        """æ‰¹é‡æ·»åŠ åˆ†ææ•°æ®"""
        analysis_ids = []
        for analysis_type, analysis_data in analyses.items():
            analysis_id = self.add(report_id, execution_id, analysis_type, analysis_data)
            analysis_ids.append(analysis_id)
        return analysis_ids
    
    def get_by_execution_id(self, execution_id: str) -> Dict[str, Any]:
        """æ ¹æ®æ‰§è¡Œ ID è·å–æ‰€æœ‰åˆ†ææ•°æ®"""
        with self.get_connection() as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute('''
                SELECT analysis_type, analysis_data
                FROM diagnosis_analysis
                WHERE execution_id = ?
            ''', (execution_id,))
            
            analysis = {}
            for row in cursor.fetchall():
                analysis_type = row['analysis_type']
                analysis_data = json.loads(row['analysis_data'])
                analysis[analysis_type] = analysis_data
            
            return analysis


# ==================== æ–‡ä»¶å½’æ¡£ç®¡ç† ====================

class FileArchiveManager:
    """
    æ–‡ä»¶å½’æ¡£ç®¡ç†å™¨
    
    èŒè´£ï¼š
    1. æ–‡ä»¶ä¿å­˜
    2. æ–‡ä»¶å½’æ¡£ï¼ˆå‹ç¼©ï¼‰
    3. æ–‡ä»¶è¯»å–
    4. æ¸…ç†æ—§æ–‡ä»¶
    """
    
    def save_report(self, execution_id: str, report_data: Dict[str, Any], 
                   created_at: Optional[datetime] = None) -> str:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if created_at is None:
            created_at = datetime.now()
        
        filepath = get_file_archive_path(execution_id, created_at)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        db_logger.info(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°æ–‡ä»¶ï¼š{filepath}")
        return filepath
    
    def archive_report(self, execution_id: str, report_data: Dict[str, Any]) -> str:
        """å½’æ¡£æŠ¥å‘Šï¼ˆå‹ç¼©ï¼‰"""
        now = datetime.now()
        archive_month = now.strftime('%Y-%m')
        archive_dir = os.path.join(ARCHIVES_DIR, archive_month)
        os.makedirs(archive_dir, exist_ok=True)
        
        filepath = os.path.join(archive_dir, f'{execution_id}.json.gz')
        
        # å‹ç¼©å†™å…¥
        with gzip.open(filepath, 'wt', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        db_logger.info(f"âœ… æŠ¥å‘Šå·²å½’æ¡£ï¼š{filepath}")
        return filepath
    
    def get_report(self, execution_id: str, created_at: datetime) -> Optional[Dict[str, Any]]:
        """ä»æ–‡ä»¶è¯»å–æŠ¥å‘Š"""
        filepath = get_file_archive_path(execution_id, created_at)
        
        if not os.path.exists(filepath):
            # å°è¯•ä»å½’æ¡£è¯»å–
            archive_month = created_at.strftime('%Y-%m')
            archive_path = os.path.join(ARCHIVES_DIR, archive_month, f'{execution_id}.json.gz')
            if os.path.exists(archive_path):
                with gzip.open(archive_path, 'rt', encoding='utf-8') as f:
                    return json.load(f)
            return None
        
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def cleanup_old_files(self, days: int = 30) -> Dict[str, Any]:
        """æ¸…ç†æ—§æ–‡ä»¶ï¼ˆç§»åŠ¨åˆ°å½’æ¡£ï¼‰"""
        cutoff_date = datetime.now() - timedelta(days=days)
        stats = {
            'archived_count': 0,
            'deleted_count': 0,
            'errors': []
        }
        
        # éå†æŠ¥å‘Šç›®å½•
        for year_dir in os.listdir(REPORTS_DIR):
            year_path = os.path.join(REPORTS_DIR, year_dir)
            if not os.path.isdir(year_path):
                continue
            
            for month_dir in os.listdir(year_path):
                month_path = os.path.join(year_path, month_dir)
                if not os.path.isdir(month_path):
                    continue
                
                for day_dir in os.listdir(month_path):
                    day_path = os.path.join(month_path, day_dir)
                    if not os.path.isdir(day_path):
                        continue
                    
                    # æ£€æŸ¥æ—¥æœŸæ˜¯å¦æ—©äºé˜ˆå€¼
                    try:
                        file_date = datetime(int(year_dir), int(month_dir), int(day_dir))
                        if file_date < cutoff_date:
                            # ç§»åŠ¨åˆ°æ­¤æœˆçš„å½’æ¡£ç›®å½•
                            archive_month = file_date.strftime('%Y-%m')
                            archive_dir = os.path.join(ARCHIVES_DIR, archive_month)
                            os.makedirs(archive_dir, exist_ok=True)
                            
                            # ç§»åŠ¨æ–‡ä»¶
                            for filename in os.listdir(day_path):
                                if filename.endswith('.json'):
                                    src = os.path.join(day_path, filename)
                                    dst = os.path.join(archive_dir, filename.replace('.json', '.json.gz'))
                                    
                                    # å‹ç¼©å¹¶ç§»åŠ¨
                                    with open(src, 'r', encoding='utf-8') as f_in:
                                        with gzip.open(dst, 'wt', encoding='utf-8') as f_out:
                                            f_out.write(f_in.read())
                                    
                                    os.remove(src)
                                    stats['archived_count'] += 1
                            
                            # åˆ é™¤ç©ºç›®å½•
                            if not os.listdir(day_path):
                                os.rmdir(day_path)
                                if not os.listdir(month_path):
                                    os.rmdir(month_path)
                                    if not os.listdir(year_path):
                                        os.rmdir(year_path)
                    
                    except Exception as e:
                        stats['errors'].append(f"{year_dir}/{month_dir}/{day_dir}: {e}")
        
        db_logger.info(f"âœ… æ¸…ç†å®Œæˆï¼šå½’æ¡£ {stats['archived_count']} ä¸ªæ–‡ä»¶")
        return stats


# ==================== ä¾¿æ·å‡½æ•° ====================

def delete_diagnosis_report_by_execution_id(execution_id: str) -> bool:
    """
    P0 ä¿®å¤ï¼šä¾¿æ·å‡½æ•° - æ ¹æ®æ‰§è¡Œ ID åˆ é™¤è¯Šæ–­æŠ¥å‘Š
    
    Args:
        execution_id: æ‰§è¡Œ ID
    
    Returns:
        bool: æ˜¯å¦åˆ é™¤æˆåŠŸ
    """
    repo = DiagnosisReportRepository()
    return repo.delete_by_execution_id(execution_id)


# ==================== åˆå§‹åŒ– ====================

def init_database_tables():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨ï¼ˆå¦‚æœå°šæœªåˆ›å»ºï¼‰"""
    with get_db_pool().get_connection() as conn:
        cursor = conn.cursor()
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='diagnosis_reports'")
        if cursor.fetchone():
            db_logger.info("âœ… æ•°æ®åº“è¡¨å·²å­˜åœ¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
            return
        
        # è¿è¡Œè¿ç§»è„šæœ¬
        migrations_dir = os.path.join(os.path.dirname(__file__), '..', 'database', 'migrations')
        migration_files = sorted([
            f for f in os.listdir(migrations_dir) 
            if f.endswith('.sql') and f.startswith('001')
        ])
        
        for migration_file in migration_files:
            with open(os.path.join(migrations_dir, migration_file), 'r', encoding='utf-8') as f:
                sql_script = f.read()
            cursor.executescript(sql_script)
        
        db_logger.info("âœ… æ•°æ®åº“è¡¨åˆå§‹åŒ–å®Œæˆ")


# æ¨¡å—åŠ è½½æ—¶åˆå§‹åŒ–
try:
    init_database_tables()
except Exception as e:
    db_logger.error(f"âš ï¸ æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥ï¼š{e}")


# ==================== ä¾¿æ·å‡½æ•° ====================

# å…¨å±€ä»“åº“å®ä¾‹
_report_repo = None


def get_diagnosis_report_repository():
    """è·å–å…¨å±€è¯Šæ–­æŠ¥å‘Šä»“åº“å®ä¾‹"""
    global _report_repo
    if _report_repo is None:
        _report_repo = DiagnosisReportRepository()
    return _report_repo


def save_diagnosis_report(
    execution_id, user_id, brand_name, competitor_brands,
    selected_models, custom_questions, status='processing',
    progress=0, stage='init', is_completed=False
):
    """
    ä¾¿æ·å‡½æ•°ï¼šä¿å­˜è¯Šæ–­æŠ¥å‘Šåˆ°æ•°æ®åº“
    """
    repo = get_diagnosis_report_repository()
    existing = repo.get_by_execution_id(execution_id)
    
    if existing:
        repo.update_status(execution_id, status, progress, stage, is_completed)
        db_logger.info(f"âœ… è¯Šæ–­æŠ¥å‘Šå·²æ›´æ–°ï¼š{execution_id}")
        return existing['id']
    else:
        config = {
            'brand_name': brand_name,
            'competitor_brands': competitor_brands,
            'selected_models': selected_models,
            'custom_questions': custom_questions
        }
        report_id = repo.create(execution_id, user_id, config)
        if is_completed:
            repo.update_status(execution_id, status, progress, stage, is_completed)
        db_logger.info(f"âœ… è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜ï¼š{execution_id}, report_id: {report_id}")
        return report_id


__all__ = [
    'DiagnosisReportRepository',
    'get_diagnosis_report_repository',
    'save_diagnosis_report',
    'calculate_checksum'
]
