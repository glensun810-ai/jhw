#!/usr/bin/env python3
"""
DS-P1-1 ä¿®å¤ï¼šæ•°æ®æ¸…ç†å’Œå½’æ¡£æœºåˆ¶

åŠŸèƒ½ï¼š
1. å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®
2. è½¯åˆ é™¤æ ‡è®°æ•°æ®å¤„ç†
3. æ•°æ®å½’æ¡£åˆ°å†å²è¡¨
4. è‡ªåŠ¨è°ƒåº¦æ‰§è¡Œ

é…ç½®ï¼š
- DATA_RETENTION_DAYS: æ•°æ®ä¿ç•™å¤©æ•°ï¼ˆé»˜è®¤ 90 å¤©ï¼‰
- CLEANUP_SCHEDULE_HOUR: æ¸…ç†æ‰§è¡Œæ—¶é—´ï¼ˆé»˜è®¤å‡Œæ™¨ 3 ç‚¹ï¼‰
"""

import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Tuple

from wechat_backend.logging_config import api_logger
from wechat_backend.database.transaction import database_transaction

# æ•°æ®ä¿ç•™ç­–ç•¥é…ç½®
DATA_RETENTION_DAYS = 90  # ä¿ç•™ 90 å¤©æ•°æ®
SOFT_DELETE_RETENTION_DAYS = 30  # è½¯åˆ é™¤æ•°æ®ä¿ç•™ 30 å¤©
ARCHIVE_THRESHOLD_DAYS = 180  # è¶…è¿‡ 180 å¤©çš„æ•°æ®å½’æ¡£

# æ•°æ®åº“è·¯å¾„
DB_PATH = Path(__file__).parent.parent / 'database.db'


def cleanup_expired_data(dry_run: bool = False) -> dict:
    """
    æ¸…ç†è¿‡æœŸæ•°æ®
    
    Args:
        dry_run: å¦‚æœä¸º Trueï¼Œåªç»Ÿè®¡ä¸åˆ é™¤
    
    Returns:
        æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
    
    Example:
        stats = cleanup_expired_data()
        print(f"åˆ é™¤äº† {stats['deleted_count']} æ¡è®°å½•")
    """
    stats = {
        'start_time': datetime.now(),
        'deleted_count': 0,
        'archived_count': 0,
        'tables_processed': [],
        'errors': []
    }
    
    cutoff_date = datetime.now() - timedelta(days=DATA_RETENTION_DAYS)
    soft_delete_cutoff = datetime.now() - timedelta(days=SOFT_DELETE_RETENTION_DAYS)
    
    api_logger.info(
        f"[DataCleanup] å¼€å§‹æ¸…ç†è¿‡æœŸæ•°æ® "
        f"(ä¿ç•™{DATA_RETENTION_DAYS}å¤©ï¼Œè½¯åˆ é™¤{SOFT_DELETE_RETENTION_DAYS}å¤©)"
    )
    
    try:
        with database_transaction("æ¸…ç†è¿‡æœŸæ•°æ®") as conn:
            cursor = conn.cursor()
            
            # 1. æ¸…ç† sync_results ä¸­çš„è½¯åˆ é™¤è®°å½•
            api_logger.info(f"[DataCleanup] æ¸…ç† sync_results è½¯åˆ é™¤è®°å½•...")
            if dry_run:
                cursor.execute("""
                    SELECT COUNT(*) FROM sync_results
                    WHERE is_deleted = 1
                    AND updated_at < ?
                """, (soft_delete_cutoff.isoformat(),))
                count = cursor.fetchone()[0]
                api_logger.info(f"[DataCleanup] [DRY RUN] å°†åˆ é™¤ {count} æ¡ sync_results è®°å½•")
            else:
                cursor.execute("""
                    DELETE FROM sync_results
                    WHERE is_deleted = 1
                    AND updated_at < ?
                """, (soft_delete_cutoff.isoformat(),))
                count = cursor.rowcount
                api_logger.info(f"[DataCleanup] å·²åˆ é™¤ {count} æ¡ sync_results è®°å½•")
            
            stats['deleted_count'] += count
            stats['tables_processed'].append('sync_results')
            
            # 2. æ¸…ç† task_statuses ä¸­çš„å·²å®Œæˆä»»åŠ¡ï¼ˆè¶…è¿‡ä¿ç•™æœŸï¼‰
            api_logger.info(f"[DataCleanup] æ¸…ç† task_statuses å·²å®Œæˆä»»åŠ¡...")
            if dry_run:
                cursor.execute("""
                    SELECT COUNT(*) FROM task_statuses
                    WHERE is_completed = 1
                    AND updated_at < ?
                """, (cutoff_date.isoformat(),))
                count = cursor.fetchone()[0]
                api_logger.info(f"[DataCleanup] [DRY RUN] å°†åˆ é™¤ {count} æ¡ task_statuses è®°å½•")
            else:
                cursor.execute("""
                    DELETE FROM task_statuses
                    WHERE is_completed = 1
                    AND updated_at < ?
                """, (cutoff_date.isoformat(),))
                count = cursor.rowcount
                api_logger.info(f"[DataCleanup] å·²åˆ é™¤ {count} æ¡ task_statuses è®°å½•")
            
            stats['deleted_count'] += count
            stats['tables_processed'].append('task_statuses')
            
            # 3. æ¸…ç† verification_codes ä¸­çš„è¿‡æœŸéªŒè¯ç 
            api_logger.info(f"[DataCleanup] æ¸…ç† verification_codes è¿‡æœŸéªŒè¯ç ...")
            if dry_run:
                cursor.execute("""
                    SELECT COUNT(*) FROM verification_codes
                    WHERE expires_at < ?
                    OR (created_at < ? AND used = 0)
                """, (datetime.now().isoformat(), cutoff_date.isoformat()))
                count = cursor.fetchone()[0]
                api_logger.info(f"[DataCleanup] [DRY RUN] å°†åˆ é™¤ {count} æ¡ verification_codes è®°å½•")
            else:
                cursor.execute("""
                    DELETE FROM verification_codes
                    WHERE expires_at < ?
                    OR (created_at < ? AND used = 0)
                """, (datetime.now().isoformat(), cutoff_date.isoformat()))
                count = cursor.rowcount
                api_logger.info(f"[DataCleanup] å·²åˆ é™¤ {count} æ¡ verification_codes è®°å½•")
            
            stats['deleted_count'] += count
            stats['tables_processed'].append('verification_codes')
            
            # 4. æ¸…ç† audit_logs ä¸­çš„è¿‡æœŸæ—¥å¿—ï¼ˆä¿ç•™ 180 å¤©ï¼‰
            audit_cutoff = datetime.now() - timedelta(days=180)
            api_logger.info(f"[DataCleanup] æ¸…ç† audit_logs è¿‡æœŸæ—¥å¿—...")
            if dry_run:
                cursor.execute("""
                    SELECT COUNT(*) FROM audit_logs
                    WHERE created_at < ?
                """, (audit_cutoff.isoformat(),))
                count = cursor.fetchone()[0]
                api_logger.info(f"[DataCleanup] [DRY RUN] å°†åˆ é™¤ {count} æ¡ audit_logs è®°å½•")
            else:
                cursor.execute("""
                    DELETE FROM audit_logs
                    WHERE created_at < ?
                """, (audit_cutoff.isoformat(),))
                count = cursor.rowcount
                api_logger.info(f"[DataCleanup] å·²åˆ é™¤ {count} æ¡ audit_logs è®°å½•")
            
            stats['deleted_count'] += count
            stats['tables_processed'].append('audit_logs')
            
            # 5. ç»Ÿè®¡æ•°æ®åº“å¤§å°å˜åŒ–
            cursor.execute("PRAGMA database_size")
            db_size = cursor.fetchone()[0] * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
            stats['database_size_bytes'] = db_size
            stats['database_size_mb'] = round(db_size / 1024 / 1024, 2)
    
    except Exception as e:
        error_msg = f"[DataCleanup] æ¸…ç†å¤±è´¥ï¼š{e}"
        api_logger.error(error_msg)
        stats['errors'].append(error_msg)
    
    # è®¡ç®—æ‰§è¡Œæ—¶é—´
    stats['end_time'] = datetime.now()
    stats['duration_seconds'] = (stats['end_time'] - stats['start_time']).total_seconds()
    
    # è®°å½•æ€»ç»“
    api_logger.info(
        f"[DataCleanup] æ¸…ç†å®Œæˆï¼šåˆ é™¤ {stats['deleted_count']} æ¡è®°å½•ï¼Œ"
        f"å½’æ¡£ {stats['archived_count']} æ¡è®°å½•ï¼Œ"
        f"è€—æ—¶ {stats['duration_seconds']:.2f}ç§’ï¼Œ"
        f"æ•°æ®åº“å¤§å° {stats['database_size_mb']}MB"
    )
    
    return stats


def archive_old_data(dry_run: bool = False) -> dict:
    """
    å½’æ¡£å†å²æ•°æ®
    
    Args:
        dry_run: å¦‚æœä¸º Trueï¼Œåªç»Ÿè®¡ä¸åˆ é™¤
    
    Returns:
        å½’æ¡£ç»Ÿè®¡ä¿¡æ¯
    """
    stats = {
        'start_time': datetime.now(),
        'archived_count': 0,
        'errors': []
    }
    
    archive_date = datetime.now() - timedelta(days=ARCHIVE_THRESHOLD_DAYS)
    
    api_logger.info(f"[DataArchive] å¼€å§‹å½’æ¡£ {ARCHIVE_THRESHOLD_DAYS} å¤©å‰çš„å†å²æ•°æ®...")
    
    try:
        with database_transaction("å½’æ¡£å†å²æ•°æ®") as conn:
            cursor = conn.cursor()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å½’æ¡£è¡¨
            cursor.execute("""
                SELECT name FROM sqlite_master
                WHERE type='table' AND name='test_records_archive'
            """)
            has_archive_table = cursor.fetchone() is not None
            
            if not has_archive_table:
                api_logger.info("[DataArchive] å½’æ¡£è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡å½’æ¡£")
                return stats
            
            # å½’æ¡£ test_records
            api_logger.info(f"[DataArchive] å½’æ¡£ test_records...")
            if dry_run:
                cursor.execute("""
                    SELECT COUNT(*) FROM test_records
                    WHERE test_date < ?
                    AND id NOT IN (SELECT id FROM test_records_archive)
                """, (archive_date.isoformat(),))
                count = cursor.fetchone()[0]
                api_logger.info(f"[DataArchive] [DRY RUN] å°†å½’æ¡£ {count} æ¡ test_records è®°å½•")
            else:
                cursor.execute("""
                    INSERT INTO test_records_archive
                    SELECT * FROM test_records
                    WHERE test_date < ?
                    AND id NOT IN (SELECT id FROM test_records_archive)
                """, (archive_date.isoformat(),))
                count = cursor.rowcount
                api_logger.info(f"[DataArchive] å·²å½’æ¡£ {count} æ¡ test_records è®°å½•")
            
            stats['archived_count'] += count
    
    except Exception as e:
        error_msg = f"[DataArchive] å½’æ¡£å¤±è´¥ï¼š{e}"
        api_logger.error(error_msg)
        stats['errors'].append(error_msg)
    
    stats['end_time'] = datetime.now()
    stats['duration_seconds'] = (stats['end_time'] - stats['start_time']).total_seconds()
    
    api_logger.info(
        f"[DataArchive] å½’æ¡£å®Œæˆï¼šå½’æ¡£ {stats['archived_count']} æ¡è®°å½•ï¼Œ"
        f"è€—æ—¶ {stats['duration_seconds']:.2f}ç§’"
    )
    
    return stats


def get_storage_stats() -> dict:
    """
    è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    stats = {}
    
    try:
        with sqlite3.connect(DB_PATH) as conn:
            cursor = conn.cursor()
            
            # æ•°æ®åº“å¤§å°
            cursor.execute("PRAGMA database_size")
            db_size = cursor.fetchone()[0] * 1024
            stats['database_size_bytes'] = db_size
            stats['database_size_mb'] = round(db_size / 1024 / 1024, 2)
            
            # å„è¡¨è®°å½•æ•°
            tables = ['test_records', 'sync_results', 'task_statuses', 'users', 'audit_logs']
            table_stats = {}
            for table in tables:
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]
                table_stats[table] = count
            
            stats['table_counts'] = table_stats
            
            # è½¯åˆ é™¤è®°å½•æ•°
            cursor.execute("""
                SELECT COUNT(*) FROM sync_results WHERE is_deleted = 1
            """)
            stats['soft_deleted_count'] = cursor.fetchone()[0]
            
            # è¿‡æœŸæ•°æ®ä¼°ç®—
            cutoff_date = (datetime.now() - timedelta(days=DATA_RETENTION_DAYS)).isoformat()
            cursor.execute("""
                SELECT COUNT(*) FROM task_statuses
                WHERE is_completed = 1 AND updated_at < ?
            """, (cutoff_date,))
            stats['expired_task_count'] = cursor.fetchone()[0]
    
    except Exception as e:
        stats['error'] = str(e)
    
    return stats


def schedule_daily_cleanup():
    """
    è°ƒåº¦æ¯æ—¥æ¸…ç†ä»»åŠ¡
    
    ä½¿ç”¨ APScheduler æ¯å¤©å‡Œæ™¨ 3 ç‚¹æ‰§è¡Œæ¸…ç†
    """
    try:
        from apscheduler.schedulers.background import BackgroundScheduler
        
        scheduler = BackgroundScheduler()
        
        # æ·»åŠ æ¯æ—¥æ¸…ç†ä»»åŠ¡
        scheduler.add_job(
            cleanup_expired_data,
            'cron',
            hour=3,
            minute=0,
            id='daily_data_cleanup',
            name='æ¯æ—¥æ•°æ®æ¸…ç†',
            replace_existing=True
        )
        
        # æ·»åŠ æ¯å‘¨å½’æ¡£ä»»åŠ¡ï¼ˆæ¯å‘¨æ—¥å‡Œæ™¨ 2 ç‚¹ï¼‰
        scheduler.add_job(
            archive_old_data,
            'cron',
            day_of_week='sun',
            hour=2,
            minute=0,
            id='weekly_data_archive',
            name='æ¯å‘¨æ•°æ®å½’æ¡£',
            replace_existing=True
        )
        
        scheduler.start()
        api_logger.info("[DataCleanup] å®šæ—¶æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨")
        
        return scheduler
    
    except ImportError:
        api_logger.warning("[DataCleanup] APScheduler æœªå®‰è£…ï¼Œæ— æ³•å¯åŠ¨å®šæ—¶ä»»åŠ¡")
        return None


if __name__ == '__main__':
    print("="*60)
    print("DS-P1-1: æ•°æ®æ¸…ç†å’Œå½’æ¡£æœºåˆ¶")
    print("="*60)
    print()
    
    # æ˜¾ç¤ºå­˜å‚¨ç»Ÿè®¡
    print("ğŸ“Š å½“å‰å­˜å‚¨ç»Ÿè®¡:")
    stats = get_storage_stats()
    print(f"  æ•°æ®åº“å¤§å°ï¼š{stats.get('database_size_mb', 'N/A')} MB")
    if 'table_counts' in stats:
        print("  å„è¡¨è®°å½•æ•°:")
        for table, count in stats['table_counts'].items():
            print(f"    {table}: {count}")
    if 'soft_deleted_count' in stats:
        print(f"  è½¯åˆ é™¤è®°å½•ï¼š{stats['soft_deleted_count']}")
    if 'expired_task_count' in stats:
        print(f"  è¿‡æœŸä»»åŠ¡ï¼š{stats['expired_task_count']}")
    
    print()
    
    # æ‰§è¡Œæ¸…ç†ï¼ˆdry run æ¨¡å¼ï¼‰
    print("ğŸ“‹ æ‰§è¡Œæ¸…ç†ï¼ˆé¢„è§ˆæ¨¡å¼ï¼‰...")
    cleanup_stats = cleanup_expired_data(dry_run=True)
    print(f"  é¢„è®¡åˆ é™¤ï¼š{cleanup_stats['deleted_count']} æ¡è®°å½•")
    print(f"  å¤„ç†è¡¨ï¼š{', '.join(cleanup_stats['tables_processed'])}")
    print(f"  è€—æ—¶ï¼š{cleanup_stats['duration_seconds']:.2f}ç§’")
    
    print()
    print("="*60)
    print("æç¤ºï¼šç”Ÿäº§ç¯å¢ƒè¯·ç§»é™¤ dry_run=True å‚æ•°æ‰§è¡Œå®é™…æ¸…ç†")
    print("="*60)
