#!/usr/bin/env python3
"""
AI å“åº”æ—¥å¿—å¤‡ä»½å’Œè¿ç§»è„šæœ¬

åŠŸèƒ½:
1. å¤‡ä»½å½“å‰æ—¥å¿—æ–‡ä»¶
2. å‹ç¼©æ—§æ—¥å¿—
3. æ¸…ç†è¶…å‡ºæ•°é‡çš„å¤‡ä»½
"""

import os
import gzip
import shutil
from datetime import datetime
from pathlib import Path

# é…ç½®
LOG_DIR = Path('/Users/sgl/PycharmProjects/PythonProject/backend_python/wechat_backend/data/ai_responses')
LOG_FILE = LOG_DIR / 'ai_responses.jsonl'
BACKUP_DIR = LOG_DIR / 'backups'
MAX_BACKUP_COUNT = 10
COMPRESSION_ENABLED = True

def create_backup():
    """åˆ›å»ºå½“å‰æ—¥å¿—æ–‡ä»¶çš„å¤‡ä»½"""
    if not LOG_FILE.exists():
        print("âš ï¸  æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤‡ä»½")
        return None
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_name = f"ai_responses_{timestamp}.jsonl"
    backup_path = BACKUP_DIR / backup_name
    
    # ç§»åŠ¨æ—¥å¿—æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•
    shutil.move(str(LOG_FILE), str(backup_path))
    print(f"âœ… å¤‡ä»½å®Œæˆï¼š{backup_name}")
    
    # å‹ç¼©å¤‡ä»½
    if COMPRESSION_ENABLED:
        compressed_path = backup_path.with_suffix('.jsonl.gz')
        with open(backup_path, 'rb') as f_in:
            with gzip.open(compressed_path, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        
        # åˆ é™¤æœªå‹ç¼©æ–‡ä»¶
        backup_path.unlink()
        
        # è®¡ç®—å‹ç¼©ç‡
        original_size = backup_path.stat().st_size if backup_path.exists() else 0
        compressed_size = compressed_path.stat().st_size
        compression_ratio = (1 - compressed_size / original_size) * 100 if original_size > 0 else 0
        
        print(f"âœ… å‹ç¼©å®Œæˆï¼š{compressed_path.name} (å‹ç¼©ç‡ï¼š{compression_ratio:.1f}%)")
        return compressed_path
    
    return backup_path

def cleanup_old_backups():
    """æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶"""
    if not BACKUP_DIR.exists():
        return
    
    # è·å–æ‰€æœ‰å¤‡ä»½æ–‡ä»¶
    backup_files = []
    for pattern in ['ai_responses_*.jsonl', 'ai_responses_*.jsonl.gz']:
        backup_files.extend(BACKUP_DIR.glob(pattern))
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    backup_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
    
    # åˆ é™¤è¶…å‡ºæ•°é‡çš„æ—§æ–‡ä»¶
    if len(backup_files) > MAX_BACKUP_COUNT:
        files_to_delete = backup_files[MAX_BACKUP_COUNT:]
        for file_path in files_to_delete:
            file_size_mb = file_path.stat().st_size / 1024 / 1024
            file_path.unlink()
            print(f"ğŸ—‘ï¸  åˆ é™¤æ—§å¤‡ä»½ï¼š{file_path.name} ({file_size_mb:.2f}MB)")
    
    # ç»Ÿè®¡
    remaining = backup_files[:MAX_BACKUP_COUNT]
    total_size = sum(f.stat().st_size for f in remaining)
    print(f"ğŸ“Š å½“å‰å¤‡ä»½ï¼š{len(remaining)}/{MAX_BACKUP_COUNT} æ–‡ä»¶ï¼Œæ€»è®¡ {total_size / 1024 / 1024:.2f}MB")

def show_stats():
    """æ˜¾ç¤ºæ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“Š æ—¥å¿—ç»Ÿè®¡:")
    print("=" * 50)
    
    # å½“å‰æ–‡ä»¶
    if LOG_FILE.exists():
        size_mb = LOG_FILE.stat().st_size / 1024 / 1024
        with open(LOG_FILE, 'r', encoding='utf-8') as f:
            lines = sum(1 for _ in f)
        print(f"å½“å‰æ–‡ä»¶ï¼š{LOG_FILE.name}")
        print(f"  å¤§å°ï¼š{size_mb:.2f}MB")
        print(f"  è®°å½•æ•°ï¼š{lines}")
    else:
        print("å½“å‰æ–‡ä»¶ï¼šä¸å­˜åœ¨")
    
    # å¤‡ä»½æ–‡ä»¶
    if BACKUP_DIR.exists():
        backup_files = []
        for pattern in ['ai_responses_*.jsonl', 'ai_responses_*.jsonl.gz']:
            backup_files.extend(BACKUP_DIR.glob(pattern))
        
        total_size = sum(f.stat().st_size for f in backup_files)
        print(f"\nå¤‡ä»½æ–‡ä»¶ï¼š{len(backup_files)} ä¸ª")
        print(f"  æ€»è®¡ï¼š{total_size / 1024 / 1024:.2f}MB")
        
        # æ˜¾ç¤ºæœ€è¿‘çš„å¤‡ä»½
        backup_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
        print("\næœ€è¿‘çš„å¤‡ä»½:")
        for f in backup_files[:5]:
            size_mb = f.stat().st_size / 1024 / 1024
            mtime = datetime.fromtimestamp(f.stat().st_mtime).strftime('%Y-%m-%d %H:%M')
            print(f"  {f.name} ({size_mb:.2f}MB) - {mtime}")
    else:
        print("\nå¤‡ä»½ç›®å½•ï¼šä¸å­˜åœ¨")

if __name__ == "__main__":
    import sys
    
    print("=" * 50)
    print("AI å“åº”æ—¥å¿—å¤‡ä»½å’Œè¿ç§»å·¥å…· V3")
    print("=" * 50)
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == 'backup':
            print("\nğŸ”„ åˆ›å»ºå¤‡ä»½...")
            create_backup()
            cleanup_old_backups()
        
        elif command == 'cleanup':
            print("\nğŸ—‘ï¸  æ¸…ç†æ—§å¤‡ä»½...")
            cleanup_old_backups()
        
        elif command == 'stats':
            show_stats()
        
        else:
            print(f"\nâŒ æœªçŸ¥å‘½ä»¤ï¼š{command}")
            print("\nå¯ç”¨å‘½ä»¤:")
            print("  backup   - åˆ›å»ºå¤‡ä»½")
            print("  cleanup  - æ¸…ç†æ—§å¤‡ä»½")
            print("  stats    - æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯")
    else:
        # é»˜è®¤æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        show_stats()
        print("\nğŸ’¡ æç¤º:")
        print("  è¿è¡Œ 'python3 migrate_to_v3.py backup' åˆ›å»ºå¤‡ä»½")
        print("  è¿è¡Œ 'python3 migrate_to_v3.py cleanup' æ¸…ç†æ—§å¤‡ä»½")
