#!/usr/bin/env python3
"""
è´Ÿé¢ä¿¡æºæå–éªŒè¯æµ‹è¯•

éªŒè¯å†…å®¹:
1. ä» AI å“åº”ä¸­æå– cited_sources
2. è¿‡æ»¤è´Ÿé¢ä¿¡æº
3. å»é‡å¤„ç†
4. ç”Ÿæˆè´Ÿé¢ä¿¡æºåˆ—è¡¨

æ‰§è¡Œï¼špython3 test_negative_sources_extraction.py
"""

import sys
sys.path.insert(0, '/Users/sgl/PycharmProjects/PythonProject/backend_python')

# æ¨¡æ‹Ÿ AI å“åº”æ•°æ®
mock_ai_responses = [
    {
        'status': 'success',
        'geo_data': {
            'rank': 1,
            'sentiment': 0.7,
            'cited_sources': [
                {'url': 'https://consumer.huawei.com/cn/phones/nova12/', 'site_name': 'åä¸ºæ¶ˆè´¹è€…å®˜ç½‘', 'attitude': 'positive'},
                {'url': 'https://zhuanlan.zhihu.com/p/123456', 'site_name': 'çŸ¥ä¹ä¸“æ  - æ‰‹æœºè¯„æµ‹', 'attitude': 'neutral'}
            ]
        }
    },
    {
        'status': 'success',
        'geo_data': {
            'rank': 2,
            'sentiment': 0.3,
            'cited_sources': [
                {'url': 'https://baike.baidu.com/item/åä¸º', 'site_name': 'ç™¾åº¦ç™¾ç§‘', 'attitude': 'neutral'},
                {'url': 'https://weibo.com/some-negative-post', 'site_name': 'å¾®åš', 'attitude': 'negative'}
            ]
        }
    },
    {
        'status': 'success',
        'geo_data': {
            'rank': -1,
            'sentiment': -0.2,
            'cited_sources': [
                {'url': 'https://zhuanlan.zhihu.com/p/negative-review', 'site_name': 'çŸ¥ä¹ - è´Ÿé¢è¯„æµ‹', 'attitude': 'negative'}
            ]
        }
    }
]

def extract_negative_sources(all_results, main_brand_score):
    """ä» AI å“åº”ä¸­æå–è´Ÿé¢ä¿¡æº"""
    negative_sources = []
    
    for result in all_results:
        if result.get('status') == 'success' and result.get('geo_data'):
            geo = result['geo_data']
            cited_sources = geo.get('cited_sources', [])
            
            for source in cited_sources:
                url = source.get('url', '')
                site_name = source.get('site_name', '')
                attitude = source.get('attitude', 'neutral')
                
                # åªæå–è´Ÿé¢æˆ–ä¸­æ€§åè´Ÿé¢çš„ä¿¡æº
                if attitude == 'negative' or (attitude == 'neutral' and main_brand_score < 70):
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡ï¼‰
                    exists = any(ns.get('source_url') == url for ns in negative_sources)
                    if not exists and url and site_name:
                        sentiment = geo.get('sentiment', 0)
                        severity = 'high' if sentiment < -0.3 else ('medium' if sentiment < 0 else 'low')
                        
                        negative_sources.append({
                            'source_name': site_name,
                            'source_url': url,
                            'source_type': 'article' if 'zhuanlan' in url or 'article' in url else 'encyclopedia' if 'baike' in url else 'social_media',
                            'content_summary': f'AI å›ç­”ä¸­å¼•ç”¨çš„ä¿¡æºï¼š{site_name}',
                            'sentiment_score': sentiment,
                            'severity': severity,
                            'impact_scope': 'medium',
                            'estimated_reach': 100000 if 'baike' in url else 50000,
                            'from_ai_response': True,
                            'attitude': attitude
                        })
    
    return negative_sources


def main():
    print("\n" + "="*70)
    print("  è´Ÿé¢ä¿¡æºæå–éªŒè¯æµ‹è¯•")
    print("  Negative Sources Extraction Test")
    print("="*70)
    
    # æµ‹è¯• 1: é«˜åˆ†å“ç‰Œï¼ˆ>70ï¼‰ï¼Œåªæå–è´Ÿé¢ä¿¡æº
    print("\n" + "="*70)
    print("æµ‹è¯• 1: é«˜åˆ†å“ç‰Œï¼ˆ85 åˆ†ï¼‰ï¼Œåªæå– attitude=negative çš„ä¿¡æº")
    print("="*70)
    
    negative_sources_high = extract_negative_sources(mock_ai_responses, 85)
    
    print(f"\n  æå–åˆ°çš„è´Ÿé¢ä¿¡æºæ•°é‡ï¼š{len(negative_sources_high)}")
    for i, ns in enumerate(negative_sources_high, 1):
        print(f"\n  [{i}] {ns['source_name']}")
        print(f"      URL: {ns['source_url']}")
        print(f"      æ€åº¦ï¼š{ns['attitude']}")
        print(f"      æƒ…æ„Ÿï¼š{ns['sentiment_score']}")
        print(f"      ç±»å‹ï¼š{ns['source_type']}")
        print(f"      æ¥è‡ª AI: {ns['from_ai_response']}")
    
    # éªŒè¯
    expected_high = 2  # å¾®åšï¼ˆnegativeï¼‰+ çŸ¥ä¹è´Ÿé¢è¯„æµ‹ï¼ˆnegativeï¼‰
    if len(negative_sources_high) == expected_high:
        print(f"\n  âœ… æµ‹è¯• 1 é€šè¿‡ï¼šæå–åˆ° {expected_high} ä¸ªè´Ÿé¢ä¿¡æº")
    else:
        print(f"\n  âŒ æµ‹è¯• 1 å¤±è´¥ï¼šé¢„æœŸ {expected_high} ä¸ªï¼Œå®é™… {len(negative_sources_high)} ä¸ª")
    
    # æµ‹è¯• 2: ä½åˆ†å“ç‰Œï¼ˆ<70ï¼‰ï¼Œæå–è´Ÿé¢å’Œä¸­æ€§ä¿¡æº
    print("\n" + "="*70)
    print("æµ‹è¯• 2: ä½åˆ†å“ç‰Œï¼ˆ55 åˆ†ï¼‰ï¼Œæå– attitude=negative å’Œ neutral çš„ä¿¡æº")
    print("="*70)
    
    negative_sources_low = extract_negative_sources(mock_ai_responses, 55)
    
    print(f"\n  æå–åˆ°çš„è´Ÿé¢ä¿¡æºæ•°é‡ï¼š{len(negative_sources_low)}")
    for i, ns in enumerate(negative_sources_low, 1):
        print(f"\n  [{i}] {ns['source_name']}")
        print(f"      URL: {ns['source_url']}")
        print(f"      æ€åº¦ï¼š{ns['attitude']}")
        print(f"      æƒ…æ„Ÿï¼š{ns['sentiment_score']}")
        print(f"      ç±»å‹ï¼š{ns['source_type']}")
    
    # éªŒè¯
    expected_low = 4  # çŸ¥ä¹ä¸“æ ï¼ˆneutralï¼‰+ ç™¾åº¦ç™¾ç§‘ï¼ˆneutralï¼‰+ å¾®åšï¼ˆnegativeï¼‰+ çŸ¥ä¹è´Ÿé¢ï¼ˆnegativeï¼‰
    if len(negative_sources_low) == expected_low:
        print(f"\n  âœ… æµ‹è¯• 2 é€šè¿‡ï¼šæå–åˆ° {expected_low} ä¸ªä¿¡æº")
    else:
        print(f"\n  âŒ æµ‹è¯• 2 å¤±è´¥ï¼šé¢„æœŸ {expected_low} ä¸ªï¼Œå®é™… {len(negative_sources_low)} ä¸ª")
    
    # æµ‹è¯• 3: å»é‡éªŒè¯
    print("\n" + "="*70)
    print("æµ‹è¯• 3: å»é‡éªŒè¯")
    print("="*70)
    
    # æ·»åŠ é‡å¤æ•°æ®
    mock_with_duplicates = mock_ai_responses + [
        {
            'status': 'success',
            'geo_data': {
                'rank': 3,
                'sentiment': -0.1,
                'cited_sources': [
                    {'url': 'https://weibo.com/some-negative-post', 'site_name': 'å¾®åš', 'attitude': 'negative'}  # é‡å¤
                ]
            }
        }
    ]
    
    negative_sources_dedup = extract_negative_sources(mock_with_duplicates, 55)
    print(f"\n  æœ‰é‡å¤è¾“å…¥çš„æå–æ•°é‡ï¼š{len(negative_sources_dedup)}")
    print(f"  æ— é‡å¤è¾“å…¥çš„æå–æ•°é‡ï¼š{len(negative_sources_low)}")
    
    if len(negative_sources_dedup) == len(negative_sources_low):
        print(f"\n  âœ… æµ‹è¯• 3 é€šè¿‡ï¼šå»é‡åŠŸèƒ½æ­£å¸¸")
    else:
        print(f"\n  âŒ æµ‹è¯• 3 å¤±è´¥ï¼šå»é‡åŠŸèƒ½å¼‚å¸¸")
    
    # æµ‹è¯• 4: ä¿¡æºç±»å‹è¯†åˆ«
    print("\n" + "="*70)
    print("æµ‹è¯• 4: ä¿¡æºç±»å‹è¯†åˆ«")
    print("="*70)
    
    source_types = {}
    for ns in negative_sources_low:
        st = ns['source_type']
        source_types[st] = source_types.get(st, 0) + 1
    
    print(f"\n  ä¿¡æºç±»å‹åˆ†å¸ƒ:")
    for st, count in source_types.items():
        print(f"    {st}: {count} ä¸ª")
    
    if 'article' in source_types and 'encyclopedia' in source_types and 'social_media' in source_types:
        print(f"\n  âœ… æµ‹è¯• 4 é€šè¿‡ï¼šä¿¡æºç±»å‹è¯†åˆ«æ­£å¸¸")
    else:
        print(f"\n  âŒ æµ‹è¯• 4 å¤±è´¥ï¼šä¿¡æºç±»å‹è¯†åˆ«å¼‚å¸¸")
    
    # æ±‡æ€»
    print("\n" + "="*70)
    print("  æµ‹è¯•æ±‡æ€»")
    print("="*70)
    
    all_passed = (
        len(negative_sources_high) == expected_high and
        len(negative_sources_low) == expected_low and
        len(negative_sources_dedup) == len(negative_sources_low)
    )
    
    if all_passed:
        print("\n  ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è´Ÿé¢ä¿¡æºæå–åŠŸèƒ½æ­£å¸¸ã€‚")
        return True
    else:
        print("\n  âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ã€‚")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
