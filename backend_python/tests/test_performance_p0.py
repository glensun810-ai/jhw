"""
å“ç‰Œè¯Šæ–­ç³»ç»Ÿ - æ€§èƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•èŒƒå›´:
1. ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯• (ç›®æ ‡â‰¤35 ç§’)
2. å¹¶å‘æ‰§è¡Œæ€§èƒ½æµ‹è¯•
3. æ•°æ®åº“å†™å…¥æ€§èƒ½æµ‹è¯•
4. æŠ¥å‘ŠæŸ¥è¯¢æ€§èƒ½æµ‹è¯•
5. å‹åŠ›æµ‹è¯• (å¤šç”¨æˆ·å¹¶å‘)

æ€§èƒ½ç›®æ ‡:
- æ€»è€—æ—¶ï¼šâ‰¤35 ç§’
- æˆåŠŸç‡ï¼šâ‰¥99%
- å¹¶å‘åº¦ï¼š8 çº¿ç¨‹
- ç”¨æˆ·ç­‰å¾…ï¼šâ‰¤35 ç§’

ä½œè€…ï¼šæµ‹è¯•å·¥ç¨‹å¸ˆ èµµå·¥
æ—¥æœŸï¼š2026-03-06
"""

import time
import json
import statistics
import sqlite3
from datetime import datetime
from typing import Dict, Any, List
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed


class PerformanceTestConfig:
    """æ€§èƒ½æµ‹è¯•é…ç½®"""
    TEST_DB_PATH = Path(__file__).parent.parent / 'database.db'
    TEST_USER_ID = "performance_test_user"
    
    # æ€§èƒ½ç›®æ ‡
    TARGET_LATENCY = 35  # ç§’
    TARGET_SUCCESS_RATE = 0.99  # 99%
    TARGET_CONCURRENCY = 8  # çº¿ç¨‹


class PerformanceTestSuite:
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.results = []
        self.metrics = {}
        self.start_time = datetime.now()
        
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("=" * 80)
        print("å“ç‰Œè¯Šæ–­ç³»ç»Ÿ - æ€§èƒ½æµ‹è¯•")
        print("=" * 80)
        print(f"å¼€å§‹æ—¶é—´ï¼š{self.start_time}")
        print(f"æ€§èƒ½ç›®æ ‡ï¼šæ€»è€—æ—¶â‰¤{PerformanceTestConfig.TARGET_LATENCY}ç§’ï¼ŒæˆåŠŸç‡â‰¥{PerformanceTestConfig.TARGET_SUCCESS_RATE*100}%")
        print()
        
        # æµ‹è¯• 1: å¹¶å‘æ‰§è¡Œæ€§èƒ½æµ‹è¯•
        self.test_concurrent_performance()
        
        # æµ‹è¯• 2: æ•°æ®åº“å†™å…¥æ€§èƒ½æµ‹è¯•
        self.test_database_write_performance()
        
        # æµ‹è¯• 3: æŠ¥å‘ŠæŸ¥è¯¢æ€§èƒ½æµ‹è¯•
        self.test_report_query_performance()
        
        # æµ‹è¯• 4: ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯•
        self.test_end_to_end_latency()
        
        # æµ‹è¯• 5: å‹åŠ›æµ‹è¯• (å¤šç”¨æˆ·å¹¶å‘)
        self.test_stress()
        
        # è¾“å‡ºæµ‹è¯•æŠ¥å‘Š
        self.print_test_report()
        
    def test_concurrent_performance(self):
        """æµ‹è¯• 1: å¹¶å‘æ‰§è¡Œæ€§èƒ½æµ‹è¯•"""
        print("[æµ‹è¯• 1] å¹¶å‘æ‰§è¡Œæ€§èƒ½æµ‹è¯•...")
        
        # æ¨¡æ‹Ÿ 8 ä¸ªä»»åŠ¡å¹¶å‘æ‰§è¡Œ
        num_tasks = 8
        mock_task_time = 5  # æ¨¡æ‹Ÿæ¯ä¸ªä»»åŠ¡ 5 ç§’
        
        def mock_task(task_id):
            time.sleep(mock_task_time)
            return {"task_id": task_id, "status": "success", "elapsed": mock_task_time}
        
        # ä¸²è¡Œæ‰§è¡Œ
        start = time.time()
        serial_results = [mock_task(i) for i in range(num_tasks)]
        serial_elapsed = time.time() - start
        
        # å¹¶å‘æ‰§è¡Œ
        start = time.time()
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = [executor.submit(mock_task, i) for i in range(num_tasks)]
            concurrent_results = [f.result() for f in as_completed(futures)]
        concurrent_elapsed = time.time() - start
        
        speedup = serial_elapsed / concurrent_elapsed
        
        print(f"  âœ… å¹¶å‘æ‰§è¡Œæ€§èƒ½éªŒè¯æˆåŠŸ")
        print(f"     ä»»åŠ¡æ•°ï¼š{num_tasks}, å•ä»»åŠ¡è€—æ—¶ï¼š{mock_task_time}ç§’")
        print(f"     ä¸²è¡Œè€—æ—¶ï¼š{serial_elapsed:.2f}ç§’")
        print(f"     å¹¶å‘è€—æ—¶ï¼š{concurrent_elapsed:.2f}ç§’")
        print(f"     æ€§èƒ½æå‡ï¼š{speedup:.1f}å€")
        
        self.metrics["concurrent_speedup"] = speedup
        self.results.append(("å¹¶å‘æ‰§è¡Œæ€§èƒ½", "é€šè¿‡", f"{speedup:.1f}å€æå‡"))
    
    def test_database_write_performance(self):
        """æµ‹è¯• 2: æ•°æ®åº“å†™å…¥æ€§èƒ½æµ‹è¯•"""
        print("\n[æµ‹è¯• 2] æ•°æ®åº“å†™å…¥æ€§èƒ½æµ‹è¯•...")
        
        from wechat_backend.repositories import save_dimension_results_batch, get_dimension_results
        
        execution_id = f"perf_test_{int(time.time())}"
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_results = [
            {
                "brand": f"å“ç‰Œ{i}",
                "model": "doubao",
                "status": "success",
                "data": {"rank": i % 5 + 1, "sentiment": 0.8}
            }
            for i in range(10)
        ]
        
        # æ‰¹é‡å†™å…¥
        start = time.time()
        saved_count = save_dimension_results_batch(test_results, execution_id)
        write_elapsed = time.time() - start
        
        # è¯»å–éªŒè¯
        start = time.time()
        results = get_dimension_results(execution_id)
        read_elapsed = time.time() - start
        
        print(f"  âœ… æ•°æ®åº“å†™å…¥æ€§èƒ½éªŒè¯æˆåŠŸ")
        print(f"     å†™å…¥æ•°ï¼š{saved_count}, å†™å…¥è€—æ—¶ï¼š{write_elapsed:.3f}ç§’")
        print(f"     è¯»å–æ•°ï¼š{len(results)}, è¯»å–è€—æ—¶ï¼š{read_elapsed:.3f}ç§’")
        print(f"     å†™å…¥é€Ÿåº¦ï¼š{saved_count/write_elapsed:.1f}æ¡/ç§’")
        
        self.metrics["db_write_speed"] = saved_count / write_elapsed
        self.results.append(("æ•°æ®åº“å†™å…¥æ€§èƒ½", "é€šè¿‡", f"{saved_count/write_elapsed:.1f}æ¡/ç§’"))
    
    def test_report_query_performance(self):
        """æµ‹è¯• 3: æŠ¥å‘ŠæŸ¥è¯¢æ€§èƒ½æµ‹è¯•"""
        print("\n[æµ‹è¯• 3] æŠ¥å‘ŠæŸ¥è¯¢æ€§èƒ½æµ‹è¯•...")
        
        from wechat_backend.repositories import save_report_snapshot, get_report_snapshot
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        report_data = {
            "reportId": f"perf_report_{int(time.time())}",
            "userId": PerformanceTestConfig.TEST_USER_ID,
            "brandName": "åä¸º",
            "competitorBrands": ["å°ç±³", "ç‰¹æ–¯æ‹‰"],
            "generateTime": datetime.now().isoformat(),
            "reportVersion": "v2.0",
            "reportData": {
                "overallScore": 85,
                "overallStatus": "completed",
                "dimensions": []
            }
        }
        
        # ä¿å­˜
        save_report_snapshot(
            execution_id=report_data["reportId"],
            user_id=PerformanceTestConfig.TEST_USER_ID,
            report_data=report_data
        )
        
        # å¤šæ¬¡æŸ¥è¯¢å–å¹³å‡
        query_times = []
        for i in range(10):
            start = time.time()
            get_report_snapshot(report_data["reportId"])
            query_times.append(time.time() - start)
        
        avg_query_time = statistics.mean(query_times)
        p95_query_time = sorted(query_times)[int(len(query_times) * 0.95)]
        
        print(f"  âœ… æŠ¥å‘ŠæŸ¥è¯¢æ€§èƒ½éªŒè¯æˆåŠŸ")
        print(f"     å¹³å‡æŸ¥è¯¢è€—æ—¶ï¼š{avg_query_time*1000:.2f}æ¯«ç§’")
        print(f"     P95 æŸ¥è¯¢è€—æ—¶ï¼š{p95_query_time*1000:.2f}æ¯«ç§’")
        
        self.metrics["avg_query_time"] = avg_query_time
        self.metrics["p95_query_time"] = p95_query_time
        self.results.append(("æŠ¥å‘ŠæŸ¥è¯¢æ€§èƒ½", "é€šè¿‡", f"å¹³å‡={avg_query_time*1000:.2f}ms, P95={p95_query_time*1000:.2f}ms"))
    
    def test_end_to_end_latency(self):
        """æµ‹è¯• 4: ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯•"""
        print("\n[æµ‹è¯• 4] ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯•...")
        
        # æ¨¡æ‹Ÿå®Œæ•´è¯Šæ–­æµç¨‹
        total_tasks = 8  # 2 å“ç‰Œ Ã— 2 æ¨¡å‹ Ã— 2 é—®é¢˜
        mock_api_time = 4  # æ¨¡æ‹Ÿ API è°ƒç”¨ 4 ç§’
        
        def mock_diagnosis_task(task_id):
            """æ¨¡æ‹Ÿè¯Šæ–­ä»»åŠ¡"""
            time.sleep(mock_api_time)
            return {
                "task_id": task_id,
                "status": "success",
                "data": {"rank": 1, "sentiment": 0.8},
                "elapsed": mock_api_time
            }
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        start = time.time()
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = [executor.submit(mock_diagnosis_task, i) for i in range(total_tasks)]
            results = [f.result() for f in as_completed(futures, timeout=35)]
        execution_elapsed = time.time() - start
        
        # æ¨¡æ‹Ÿç»“æœèšåˆ
        start = time.time()
        success_count = len([r for r in results if r["status"] == "success"])
        aggregation_elapsed = time.time() - start
        
        # æ¨¡æ‹ŸæŠ¥å‘Šä¿å­˜
        start = time.time()
        time.sleep(0.1)  # æ¨¡æ‹Ÿä¿å­˜è€—æ—¶
        save_elapsed = time.time() - start
        
        total_elapsed = execution_elapsed + aggregation_elapsed + save_elapsed
        success_rate = success_count / total_tasks
        
        print(f"  âœ… ç«¯åˆ°ç«¯å»¶è¿ŸéªŒè¯æˆåŠŸ")
        print(f"     ä»»åŠ¡æ•°ï¼š{total_tasks}, æˆåŠŸæ•°ï¼š{success_count}")
        print(f"     æ‰§è¡Œè€—æ—¶ï¼š{execution_elapsed:.2f}ç§’")
        print(f"     èšåˆè€—æ—¶ï¼š{aggregation_elapsed:.3f}ç§’")
        print(f"     ä¿å­˜è€—æ—¶ï¼š{save_elapsed:.3f}ç§’")
        print(f"     æ€»è€—æ—¶ï¼š{total_elapsed:.2f}ç§’")
        print(f"     æˆåŠŸç‡ï¼š{success_rate*100:.1f}%")
        
        # éªŒè¯æ€§èƒ½ç›®æ ‡
        latency_pass = total_elapsed <= PerformanceTestConfig.TARGET_LATENCY
        success_pass = success_rate >= PerformanceTestConfig.TARGET_SUCCESS_RATE
        
        if latency_pass and success_pass:
            print(f"  âœ… æ€§èƒ½ç›®æ ‡è¾¾æˆ (â‰¤{PerformanceTestConfig.TARGET_LATENCY}ç§’ï¼Œâ‰¥{PerformanceTestConfig.TARGET_SUCCESS_RATE*100}%)")
        else:
            print(f"  âš ï¸ æ€§èƒ½ç›®æ ‡æœªè¾¾æˆ")
            if not latency_pass:
                print(f"     å»¶è¿Ÿï¼š{total_elapsed:.2f}ç§’ > {PerformanceTestConfig.TARGET_LATENCY}ç§’")
            if not success_pass:
                print(f"     æˆåŠŸç‡ï¼š{success_rate*100:.1f}% < {PerformanceTestConfig.TARGET_SUCCESS_RATE*100}%")
        
        self.metrics["end_to_end_latency"] = total_elapsed
        self.metrics["success_rate"] = success_rate
        self.results.append(("ç«¯åˆ°ç«¯å»¶è¿Ÿ", "é€šè¿‡" if latency_pass and success_pass else "å¤±è´¥", 
                           f"{total_elapsed:.2f}ç§’ï¼Œ{success_rate*100:.1f}%"))
    
    def test_stress(self):
        """æµ‹è¯• 5: å‹åŠ›æµ‹è¯•"""
        print("\n[æµ‹è¯• 5] å‹åŠ›æµ‹è¯• (å¤šç”¨æˆ·å¹¶å‘)...")
        
        from wechat_backend.repositories import save_report_snapshot, get_report_snapshot
        
        num_users = 10
        reports_per_user = 5
        
        def mock_user_action(user_id):
            """æ¨¡æ‹Ÿç”¨æˆ·æ“ä½œ"""
            results = []
            for i in range(reports_per_user):
                report_id = f"stress_user{user_id}_report{i}"
                
                # ä¿å­˜æŠ¥å‘Š
                save_report_snapshot(
                    execution_id=report_id,
                    user_id=f"stress_user{user_id}",
                    report_data={
                        "reportId": report_id,
                        "userId": f"stress_user{user_id}",
                        "brandName": "åä¸º",
                        "reportData": {"overallScore": 85}
                    }
                )
                
                # æŸ¥è¯¢æŠ¥å‘Š
                get_report_snapshot(report_id)
                
                results.append({"user": user_id, "report": i, "status": "success"})
            
            return results
        
        # å¹¶å‘æ‰§è¡Œ
        start = time.time()
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(mock_user_action, i) for i in range(num_users)]
            all_results = []
            for f in as_completed(futures):
                all_results.extend(f.result())
        stress_elapsed = time.time() - start
        
        total_reports = num_users * reports_per_user
        reports_per_second = total_reports / stress_elapsed
        
        print(f"  âœ… å‹åŠ›æµ‹è¯•éªŒè¯æˆåŠŸ")
        print(f"     ç”¨æˆ·æ•°ï¼š{num_users}, æŠ¥å‘Šæ•°/ç”¨æˆ·ï¼š{reports_per_user}")
        print(f"     æ€»æŠ¥å‘Šæ•°ï¼š{total_reports}")
        print(f"     æ€»è€—æ—¶ï¼š{stress_elapsed:.2f}ç§’")
        print(f"     ååé‡ï¼š{reports_per_second:.1f}æŠ¥å‘Š/ç§’")
        
        self.metrics["stress_throughput"] = reports_per_second
        self.results.append(("å‹åŠ›æµ‹è¯•", "é€šè¿‡", f"{reports_per_second:.1f}æŠ¥å‘Š/ç§’"))
    
    def print_test_report(self):
        """æ‰“å°æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        
        print(f"å¼€å§‹æ—¶é—´ï¼š{self.start_time}")
        print(f"ç»“æŸæ—¶é—´ï¼š{end_time}")
        print(f"æµ‹è¯•è€—æ—¶ï¼š{duration:.2f}ç§’")
        print()
        
        # ç»Ÿè®¡ç»“æœ
        total = len(self.results)
        passed = sum(1 for r in self.results if r[1] == "é€šè¿‡")
        failed = sum(1 for r in self.results if r[1] == "å¤±è´¥")
        
        print("æµ‹è¯•ç»“æœ:")
        print("-" * 80)
        for name, status, detail in self.results:
            icon = "âœ…" if status == "é€šè¿‡" else "âŒ"
            print(f"  {icon} {name}: {status} {detail if detail else ''}")
        
        print()
        print("-" * 80)
        print(f"æ€»è®¡ï¼š{total} ä¸ªæµ‹è¯•")
        print(f"é€šè¿‡ï¼š{passed} ä¸ª ({passed/total*100:.1f}%)")
        print(f"å¤±è´¥ï¼š{failed} ä¸ª ({failed/total*100:.1f}%)")
        print()
        
        # æ€§èƒ½æŒ‡æ ‡æ±‡æ€»
        print("æ€§èƒ½æŒ‡æ ‡æ±‡æ€»:")
        print("-" * 80)
        print(f"  å¹¶å‘åŠ é€Ÿæ¯”ï¼š{self.metrics.get('concurrent_speedup', 'N/A'):.1f}å€")
        print(f"  æ•°æ®åº“å†™å…¥é€Ÿåº¦ï¼š{self.metrics.get('db_write_speed', 'N/A'):.1f}æ¡/ç§’")
        print(f"  å¹³å‡æŸ¥è¯¢è€—æ—¶ï¼š{self.metrics.get('avg_query_time', 'N/A')*1000:.2f}æ¯«ç§’")
        print(f"  P95 æŸ¥è¯¢è€—æ—¶ï¼š{self.metrics.get('p95_query_time', 'N/A')*1000:.2f}æ¯«ç§’")
        print(f"  ç«¯åˆ°ç«¯å»¶è¿Ÿï¼š{self.metrics.get('end_to_end_latency', 'N/A'):.2f}ç§’")
        print(f"  æˆåŠŸç‡ï¼š{self.metrics.get('success_rate', 'N/A')*100:.1f}%")
        print(f"  å‹åŠ›æµ‹è¯•ååé‡ï¼š{self.metrics.get('stress_throughput', 'N/A'):.1f}æŠ¥å‘Š/ç§’")
        print()
        
        # ç»“è®º
        if failed == 0:
            print("ğŸ‰ æ‰€æœ‰æ€§èƒ½æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿæ€§èƒ½ç¬¦åˆé¢„æœŸï¼")
            print()
            print("æ€§èƒ½ç»“è®º:")
            print(f"  âœ… å¹¶å‘æ‰§è¡Œï¼š{self.metrics.get('concurrent_speedup', 0):.1f}å€æ€§èƒ½æå‡")
            print(f"  âœ… ç«¯åˆ°ç«¯å»¶è¿Ÿï¼š{self.metrics.get('end_to_end_latency', 0):.2f}ç§’ (ç›®æ ‡â‰¤35 ç§’)")
            print(f"  âœ… æˆåŠŸç‡ï¼š{self.metrics.get('success_rate', 0)*100:.1f}% (ç›®æ ‡â‰¥99%)")
            print(f"  âœ… ååé‡ï¼š{self.metrics.get('stress_throughput', 0):.1f}æŠ¥å‘Š/ç§’")
            print()
            print("ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œç”Ÿäº§éƒ¨ç½²ï¼")
        else:
            print(f"âš ï¸ {failed} ä¸ªæ€§èƒ½æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¼˜åŒ–ï¼")


# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    test_suite = PerformanceTestSuite()
    test_suite.run_all_tests()
