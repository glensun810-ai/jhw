#!/usr/bin/env python3
"""
æ‰¹é‡æ›´æ–°AIé€‚é…å™¨ä»¥åº”ç”¨å®‰å…¨æ”¹è¿›æªæ–½
"""

import os
from pathlib import Path


def update_chatgpt_adapter():
    """æ›´æ–°ChatGPTé€‚é…å™¨ä»¥åº”ç”¨å®‰å…¨åŠŸèƒ½"""
    
    chatgpt_content = '''import time
import requests
from typing import Optional
from ..logging_config import api_logger
from .base_adapter import AIClient, AIResponse, AIPlatformType, AIErrorType
from ..network.request_wrapper import get_ai_request_wrapper
from ..monitoring.metrics_collector import record_api_call, record_error
from ..monitoring.logging_enhancements import log_api_request, log_api_response
from config_manager import Config as PlatformConfigManager

class ChatGPTAdapter(AIClient):
    """
    ChatGPT (OpenAI) AI å¹³å°çš„é€‚é…å™¨
    """
    def __init__(self, api_key: str, model_name: str = "gpt-3.5-turbo", base_url: Optional[str] = None):
        super().__init__(AIPlatformType.CHATGPT, model_name, api_key)
        
        # ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚å°è£…å™¨
        self.request_wrapper = get_ai_request_wrapper(
            platform_name="chatgpt",
            base_url=base_url or "https://api.openai.com/v1",
            api_key=api_key,
            timeout=30,
            max_retries=3
        )
        
        api_logger.info(f"ChatGPTAdapter initialized for model: {model_name} with unified request wrapper")

    def send_prompt(self, prompt: str, **kwargs) -> AIResponse:
        """
        å‘ ChatGPT API å‘é€è¯·æ±‚
        """
        messages = [{"role": "user", "content": prompt}]

        platform_config_manager = PlatformConfigManager()
        chatgpt_config = platform_config_manager.get_platform_config('chatgpt')

        temperature = kwargs.get('temperature', chatgpt_config.default_temperature if chatgpt_config else 0.7)
        max_tokens = kwargs.get('max_tokens', chatgpt_config.default_max_tokens if chatgpt_config else 1000)
        timeout = kwargs.get('timeout', chatgpt_config.timeout if chatgpt_config else 30)

        payload = {
            "model": self.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }

        start_time = time.time()

        try:
            # ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚å°è£…å™¨å‘é€è¯·æ±‚
            response = self.request_wrapper.make_ai_request(
                endpoint="/chat/completions",
                prompt=prompt,
                model=self.model_name,
                json=payload,
                timeout=timeout
            )

            response.raise_for_status()
            response_data = response.json()
            latency = time.time() - start_time

            if response_data and response_data.get("choices"):
                content = response_data["choices"][0]["message"]["content"]
                tokens_used = response_data["usage"]["total_tokens"] if response_data.get("usage") else 0
                api_logger.info(f"ChatGPT response success. Tokens: {tokens_used}, Latency: {latency:.2f}s")
                return AIResponse(
                    success=True,
                    content=content,
                    model=self.model_name,
                    platform=self.platform_type.value,
                    tokens_used=tokens_used,
                    latency=latency,
                    metadata=response_data
                )
            else:
                error_message = response_data.get("error", {}).get("message", "Unknown ChatGPT API error")
                error_type = self._map_error_message(error_message)
                api_logger.error(f"ChatGPT API returned no choices: {error_message}")
                return AIResponse(
                    success=False, 
                    error_message=error_message, 
                    error_type=error_type, 
                    model=self.model_name, 
                    platform=self.platform_type.value, 
                    latency=latency
                )

        except requests.exceptions.RequestException as e:
            error_message = f"ChatGPT API request failed: {e}"
            error_type = AIErrorType.UNKNOWN_ERROR
            latency = time.time() - start_time
            
            if hasattr(e, 'response') and e.response is not None:
                status_code = e.response.status_code
                if status_code == 401:
                    error_type = AIErrorType.INVALID_API_KEY
                elif status_code == 429:
                    error_type = AIErrorType.RATE_LIMIT_EXCEEDED
                elif status_code >= 500:
                    error_type = AIErrorType.SERVER_ERROR
                elif status_code == 403:
                    error_type = AIErrorType.INVALID_API_KEY

            api_logger.error(error_message)
            record_error("chatgpt", error_type.value, str(e))
            return AIResponse(
                success=False, 
                error_message=error_message, 
                error_type=error_type, 
                model=self.model_name, 
                platform=self.platform_type.value, 
                latency=latency
            )
        except Exception as e:
            error_message = f"An unexpected error occurred with ChatGPT API: {e}"
            latency = time.time() - start_time
            api_logger.error(error_message)
            record_error("chatgpt", "UNKNOWN_ERROR", str(e))
            return AIResponse(
                success=False, 
                error_message=error_message, 
                error_type=AIErrorType.UNKNOWN_ERROR, 
                model=self.model_name, 
                platform=self.platform_type.value, 
                latency=latency
            )

    def _map_error_message(self, error_message: str) -> AIErrorType:
        """å°†ChatGPTçš„é”™è¯¯ä¿¡æ¯æ˜ å°„åˆ°æ ‡å‡†é”™è¯¯ç±»å‹"""
        error_message_lower = error_message.lower()
        if "incorrect api key" in error_message_lower:
            return AIErrorType.INVALID_API_KEY
        if "quota" in error_message_lower:
            return AIErrorType.INSUFFICIENT_QUOTA
        if "content policy" in error_message_lower:
            return AIErrorType.CONTENT_SAFETY
        return AIErrorType.UNKNOWN_ERROR
'''
    
    # æ›´æ–°ChatGPTé€‚é…å™¨
    adapter_path = Path('wechat_backend/ai_adapters/chatgpt_adapter.py')
    with open(adapter_path, 'w', encoding='utf-8') as f:
        f.write(chatgpt_content)
    
    print("âœ“ å·²æ›´æ–°ChatGPTé€‚é…å™¨ä»¥åº”ç”¨å®‰å…¨åŠŸèƒ½")


def update_qwen_adapter():
    """æ›´æ–°Qwené€‚é…å™¨ä»¥åº”ç”¨å®‰å…¨åŠŸèƒ½"""
    
    qwen_content = '''import time
import requests
from typing import Optional
from ..logging_config import api_logger
from .base_adapter import AIClient, AIResponse, AIPlatformType, AIErrorType
from ..network.request_wrapper import get_ai_request_wrapper
from ..monitoring.metrics_collector import record_api_call, record_error
from ..monitoring.logging_enhancements import log_api_request, log_api_response
from config_manager import Config as PlatformConfigManager

class QwenAdapter(AIClient):
    """
    Qwen (Alibaba Tongyi) AI å¹³å°çš„é€‚é…å™¨
    """
    def __init__(self, api_key: str, model_name: str = "qwen-max", base_url: Optional[str] = None):
        super().__init__(AIPlatformType.QWEN, model_name, api_key)
        
        # ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚å°è£…å™¨
        self.request_wrapper = get_ai_request_wrapper(
            platform_name="qwen",
            base_url=base_url or "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
            api_key=api_key,
            timeout=30,
            max_retries=3
        )
        
        api_logger.info(f"QwenAdapter initialized for model: {model_name} with unified request wrapper")

    def send_prompt(self, prompt: str, **kwargs) -> AIResponse:
        """
        å‘ Qwen API å‘é€è¯·æ±‚
        """
        messages = [{"role": "user", "content": prompt}]

        platform_config_manager = PlatformConfigManager()
        qwen_config = platform_config_manager.get_platform_config('qwen')

        temperature = kwargs.get('temperature', qwen_config.default_temperature if qwen_config else 0.7)

        payload = {
            "model": self.model_name,
            "input": {
                "messages": messages
            },
            "parameters": {
                "temperature": temperature,
            }
        }

        start_time = time.time()
        try:
            # ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚å°è£…å™¨å‘é€è¯·æ±‚
            response = self.request_wrapper.make_ai_request(
                endpoint="",  # Qwen API endpoint is specified in base_url
                prompt=prompt,
                model=self.model_name,
                json=payload,
                timeout=30
            )
            
            response.raise_for_status()
            response_data = response.json()
            latency = time.time() - start_time

            if response_data and response_data.get("output"):
                content = response_data["output"]["text"]
                tokens_used = response_data["usage"]["total_tokens"] if response_data.get("usage") else 0
                api_logger.info(f"Qwen response success. Tokens: {tokens_used}, Latency: {latency:.2f}s")
                return AIResponse(
                    success=True,
                    content=content,
                    model=self.model_name,
                    platform=self.platform_type.value,
                    tokens_used=tokens_used,
                    latency=latency,
                    metadata=response_data
                )
            else:
                error_code = response_data.get("code", "")
                error_message = response_data.get("message", "Unknown Qwen API error")
                error_type = self._map_error_code(error_code)

                api_logger.error(f"Qwen API returned no output: {error_code} - {error_message}")
                return AIResponse(
                    success=False, 
                    error_message=error_message, 
                    error_type=error_type, 
                    model=self.model_name, 
                    platform=self.platform_type.value, 
                    latency=latency
                )

        except requests.exceptions.RequestException as e:
            error_message = f"Qwen API request failed: {e}"
            api_logger.error(error_message)
            latency = time.time() - start_time
            record_error("qwen", "REQUEST_EXCEPTION", str(e))
            return AIResponse(
                success=False, 
                error_message=error_message, 
                error_type=AIErrorType.UNKNOWN_ERROR, 
                model=self.model_name, 
                platform=self.platform_type.value, 
                latency=latency
            )
        except Exception as e:
            error_message = f"An unexpected error occurred with Qwen API: {e}"
            api_logger.error(error_message)
            latency = time.time() - start_time
            record_error("qwen", "UNKNOWN_ERROR", str(e))
            return AIResponse(
                success=False, 
                error_message=error_message, 
                error_type=AIErrorType.UNKNOWN_ERROR, 
                model=self.model_name, 
                platform=self.platform_type.value, 
                latency=latency
            )

    def _map_error_code(self, error_code: str) -> AIErrorType:
        """å°†Qwençš„é”™è¯¯ç æ˜ å°„åˆ°æ ‡å‡†é”™è¯¯ç±»å‹"""
        if error_code == "InvalidAPIKey":
            return AIErrorType.INVALID_API_KEY
        if error_code == "QuotaExhausted":
            return AIErrorType.INSUFFICIENT_QUOTA
        if error_code == "OperationDenied.ContentRisk":
            return AIErrorType.CONTENT_SAFETY
        if "Throttling" in error_code:
            return AIErrorType.RATE_LIMIT_EXCEEDED
        return AIErrorType.UNKNOWN_ERROR
'''
    
    # æ›´æ–°Qwené€‚é…å™¨
    adapter_path = Path('wechat_backend/ai_adapters/qwen_adapter.py')
    with open(adapter_path, 'w', encoding='utf-8') as f:
        f.write(qwen_content)
    
    print("âœ“ å·²æ›´æ–°Qwené€‚é…å™¨ä»¥åº”ç”¨å®‰å…¨åŠŸèƒ½")


def update_gemini_adapter():
    """æ›´æ–°Geminié€‚é…å™¨ä»¥åº”ç”¨å®‰å…¨åŠŸèƒ½"""
    
    gemini_content = '''import time
import requests
from typing import Optional
from ..logging_config import api_logger
from .base_adapter import AIClient, AIResponse, AIPlatformType, AIErrorType
from ..network.request_wrapper import get_ai_request_wrapper
from ..monitoring.metrics_collector import record_api_call, record_error
from ..monitoring.logging_enhancements import log_api_request, log_api_response
from config_manager import Config as PlatformConfigManager

class GeminiAdapter(AIClient):
    """
    Google Gemini AI å¹³å°çš„é€‚é…å™¨ - ä½¿ç”¨ REST API é¿å… Python ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
    """
    def __init__(self, api_key: str, model_name: str = "gemini-pro", base_url: Optional[str] = None):
        super().__init__(AIPlatformType.GEMINI, model_name, api_key)

        # ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚å°è£…å™¨
        self.request_wrapper = get_ai_request_wrapper(
            platform_name="gemini",
            base_url=base_url or f"https://generativelanguage.googleapis.com/v1beta/models",
            api_key=api_key,
            timeout=30,
            max_retries=3
        )

        api_logger.info(f"GeminiAdapter initialized for model: {model_name} using REST API with unified request wrapper")

    def send_prompt(self, prompt: str, **kwargs) -> AIResponse:
        """
        å‘ Gemini API å‘é€è¯·æ±‚ - ä½¿ç”¨ REST API
        """
        payload = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }],
            "generationConfig": {
                "temperature": kwargs.get('temperature', 0.7),
                "maxOutputTokens": kwargs.get('max_tokens', 2048)
            }
        }

        start_time = time.time()
        try:
            # ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚å°è£…å™¨å‘é€è¯·æ±‚
            url_suffix = f"/{self.model_name}:generateContent?key={self.api_key}"
            response = self.request_wrapper.make_ai_request(
                endpoint=url_suffix,
                prompt=prompt,
                model=self.model_name,
                json=payload,
                timeout=30
            )

            response.raise_for_status()
            response_data = response.json()
            latency = time.time() - start_time

            # è§£æå“åº”
            candidates = response_data.get("candidates", [])
            if candidates:
                content = candidates[0]["content"]["parts"][0]["text"]

                # ä¼°ç®— token æ•°é‡
                tokens_used = len(prompt.split()) + len(content.split())

                api_logger.info(f"Gemini response success. Tokens: ~{tokens_used}, Latency: {latency:.2f}s")
                return AIResponse(
                    success=True,
                    content=content,
                    model=self.model_name,
                    platform=self.platform_type.value,
                    tokens_used=tokens_used,
                    latency=latency,
                    metadata=response_data
                )
            else:
                error_details = response_data.get("error", {})
                error_message = error_details.get("message", "Unknown Gemini API error")
                error_type = self._map_error_code(error_details.get("code", 0))

                api_logger.error(f"Gemini API returned no candidates: {error_message}")
                return AIResponse(
                    success=False,
                    error_message=error_message,
                    error_type=error_type,
                    model=self.model_name,
                    platform=self.platform_type.value,
                    latency=latency
                )

        except requests.exceptions.RequestException as e:
            latency = time.time() - start_time
            error_message = f"Gemini API request failed: {e}"
            error_type = AIErrorType.UNKNOWN_ERROR

            if hasattr(e, 'response') and e.response is not None:
                status_code = e.response.status_code
                if status_code == 400:
                    error_type = AIErrorType.CONTENT_SAFETY  # Often safety issues return 400
                elif status_code == 401 or status_code == 403:
                    error_type = AIErrorType.INVALID_API_KEY
                elif status_code == 429:
                    error_type = AIErrorType.INSUFFICIENT_QUOTA  # Quota issues often return 429
                elif status_code >= 500:
                    error_type = AIErrorType.SERVER_ERROR

            api_logger.error(error_message)
            record_error("gemini", error_type.value, str(e))
            return AIResponse(
                success=False,
                error_message=error_message,
                error_type=error_type,
                model=self.model_name,
                platform=self.platform_type.value,
                latency=latency
            )
        except Exception as e:
            latency = time.time() - start_time
            error_message = f"An unexpected error occurred with Gemini API: {e}"
            api_logger.error(error_message)
            record_error("gemini", "UNKNOWN_ERROR", str(e))
            return AIResponse(
                success=False,
                error_message=error_message,
                error_type=AIErrorType.UNKNOWN_ERROR,
                model=self.model_name,
                platform=self.platform_type.value,
                latency=latency
            )

    def _map_error_code(self, error_code: int) -> AIErrorType:
        """å°†Geminiçš„é”™è¯¯ç æ˜ å°„åˆ°æ ‡å‡†é”™è¯¯ç±»å‹"""
        if error_code == 3:  # Invalid argument (often means invalid API key)
            return AIErrorType.INVALID_API_KEY
        if error_code == 7:  # Permission denied
            return AIErrorType.INVALID_API_KEY
        if error_code == 8:  # Resource exhausted (quota)
            return AIErrorType.INSUFFICIENT_QUOTA
        if error_code == 10:  # Failed precondition (could be safety)
            return AIErrorType.CONTENT_SAFETY
        if error_code == 13:  # Internal server error
            return AIErrorType.SERVER_ERROR
        if error_code == 14:  # Service unavailable
            return AIErrorType.SERVER_ERROR
        return AIErrorType.UNKNOWN_ERROR
'''
    
    # æ›´æ–°Geminié€‚é…å™¨
    adapter_path = Path('wechat_backend/ai_adapters/gemini_adapter.py')
    with open(adapter_path, 'w', encoding='utf-8') as f:
        f.write(gemini_content)
    
    print("âœ“ å·²æ›´æ–°Geminié€‚é…å™¨ä»¥åº”ç”¨å®‰å…¨åŠŸèƒ½")


def main():
    print("ğŸ”„ å¼€å§‹æ‰¹é‡æ›´æ–°AIé€‚é…å™¨ä»¥åº”ç”¨å®‰å…¨æ”¹è¿›æªæ–½")
    print("=" * 60)
    
    print("\n1. æ›´æ–°ChatGPTé€‚é…å™¨...")
    update_chatgpt_adapter()
    
    print("\n2. æ›´æ–°Qwené€‚é…å™¨...")
    update_qwen_adapter()
    
    print("\n3. æ›´æ–°Geminié€‚é…å™¨...")
    update_gemini_adapter()
    
    print("\n" + "=" * 60)
    print("âœ… æ‰¹é‡æ›´æ–°å®Œæˆï¼")
    print("\nå·²æ›´æ–°çš„é€‚é…å™¨ï¼š")
    print("â€¢ ChatGPTé€‚é…å™¨ - ç°åœ¨ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚å°è£…å™¨")
    print("â€¢ Qwené€‚é…å™¨ - ç°åœ¨ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚å°è£…å™¨") 
    print("â€¢ Geminié€‚é…å™¨ - ç°åœ¨ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚å°è£…å™¨")
    print("\næ‰€æœ‰æ›´æ–°çš„é€‚é…å™¨ç°åœ¨éƒ½å…·å¤‡ï¼š")
    print("â€¢ ç»Ÿä¸€è¯·æ±‚å°è£…")
    print("â€¢ æ–­è·¯å™¨ä¿æŠ¤")
    print("â€¢ é‡è¯•æœºåˆ¶")
    print("â€¢ é€Ÿç‡é™åˆ¶")
    print("â€¢ æŒ‡æ ‡æ”¶é›†")
    print("â€¢ ç»“æ„åŒ–æ—¥å¿—è®°å½•")


if __name__ == "__main__":
    main()