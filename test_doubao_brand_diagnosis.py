#!/usr/bin/env python3
"""
æµ‹è¯•è±†åŒ…APIä¸å“ç‰Œè¯Šæ–­åŠŸèƒ½çš„å®Œæ•´é›†æˆ
"""

import os
import sys
import json
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def test_brand_diagnosis_workflow():
    """æµ‹è¯•å“ç‰Œè¯Šæ–­å®Œæ•´å·¥ä½œæµç¨‹"""
    print("ğŸ” æµ‹è¯•å“ç‰Œè¯Šæ–­å®Œæ•´å·¥ä½œæµç¨‹...")
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    try:
        from wechat_backend.ai_adapters.factory import AIAdapterFactory
        from wechat_backend.ai_adapters.base_adapter import AIPlatformType
        from wechat_backend.question_system import QuestionManager, TestCaseGenerator
        from wechat_backend.test_engine import TestExecutor, ExecutionStrategy
        from wechat_backend.ai_utils import run_brand_test_with_ai
        from ai_judge_module import AIJudgeClient
        from scoring_engine import ScoringEngine
        from enhanced_scoring_engine import EnhancedScoringEngine, calculate_enhanced_scores
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•æ•°æ®
    brand_list = ["æµ‹è¯•å“ç‰ŒA", "ç«å“å“ç‰ŒB"]
    selected_models = [{"name": "è±†åŒ…", "checked": True}]  # ä½¿ç”¨è±†åŒ…å¹³å°
    custom_questions = [
        "ä»‹ç»ä¸€ä¸‹{brandName}",
        "{brandName}çš„ä¸»è¦äº§å“æ˜¯ä»€ä¹ˆï¼Ÿ",
        "{brandName}å’Œç«å“æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
    ]
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®:")
    print(f"   å“ç‰Œåˆ—è¡¨: {brand_list}")
    print(f"   é€‰æ‹©æ¨¡å‹: {[m['name'] for m in selected_models]}")
    print(f"   è‡ªå®šä¹‰é—®é¢˜: {len(custom_questions)} ä¸ª")
    
    # 1. æµ‹è¯•é€‚é…å™¨æ˜¯å¦å¯ä»¥æ­£å¸¸ä½¿ç”¨
    print("\n1ï¸âƒ£ æµ‹è¯•è±†åŒ…é€‚é…å™¨...")
    try:
        api_key = os.getenv('DOUBAO_API_KEY')
        model_id = os.getenv('DOUBAO_MODEL_ID', 'ep-20260212000000-gd5tq')
        
        adapter = AIAdapterFactory.create(AIPlatformType.DOUBAO, api_key, model_id)
        print(f"   âœ… é€‚é…å™¨åˆ›å»ºæˆåŠŸï¼Œæ¨¡å‹: {adapter.model_name}")
        
        # å‘é€æµ‹è¯•è¯·æ±‚
        test_prompt = f"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹{brand_list[0]}ï¼Œç”¨ä¸€å¥è¯å›ç­”ã€‚"
        response = adapter.send_prompt(test_prompt)
        
        if response.success:
            print(f"   âœ… APIè¯·æ±‚æˆåŠŸ")
            print(f"   ğŸ“ å“åº”é¢„è§ˆ: {response.content[:50]}...")
        else:
            print(f"   âŒ APIè¯·æ±‚å¤±è´¥: {response.error_message}")
            return False
    except Exception as e:
        print(f"   âŒ é€‚é…å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # 2. æµ‹è¯•é—®é¢˜ç”Ÿæˆ
    print("\n2ï¸âƒ£ æµ‹è¯•é—®é¢˜ç”Ÿæˆ...")
    try:
        question_manager = QuestionManager()
        test_case_generator = TestCaseGenerator()
        
        # ä¸ºæ¯ä¸ªå“ç‰Œç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        all_test_cases = []
        for brand in brand_list:
            brand_questions = [q.replace('{brandName}', brand) for q in custom_questions]
            cases = test_case_generator.generate_test_cases(brand, selected_models, brand_questions)
            all_test_cases.extend(cases)
        
        print(f"   âœ… ç”Ÿæˆäº† {len(all_test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        for i, case in enumerate(all_test_cases[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"      [{i+1}] {case.brand_name} - {case.question}")
        if len(all_test_cases) > 3:
            print(f"      ... è¿˜æœ‰ {len(all_test_cases)-3} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    except Exception as e:
        print(f"   âŒ é—®é¢˜ç”Ÿæˆå¤±è´¥: {e}")
        return False
    
    # 3. æµ‹è¯•æ‰§è¡Œå™¨
    print("\n3ï¸âƒ£ æµ‹è¯•æµ‹è¯•æ‰§è¡Œå™¨...")
    try:
        executor = TestExecutor(max_workers=5, strategy=ExecutionStrategy.CONCURRENT)
        print(f"   âœ… æ‰§è¡Œå™¨åˆ›å»ºæˆåŠŸ")
        
        # ä¸ºäº†æµ‹è¯•ï¼Œæˆ‘ä»¬åªæ‰§è¡Œç¬¬ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹
        if all_test_cases:
            print(f"   ğŸ§ª å‡†å¤‡æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹: {len(all_test_cases)} ä¸ª")

            # æ‰§è¡Œæµ‹è¯•
            def dummy_progress_callback(execution_id, progress):
                print(f"      è¿›åº¦: {progress.progress_percentage:.1f}% ({progress.completed_tests}/{progress.total_tests})")

            results = executor.execute_tests(all_test_cases[:1], api_key, dummy_progress_callback)  # åªæ‰§è¡Œç¬¬ä¸€ä¸ª
            print(f"   âœ… æµ‹è¯•æ‰§è¡Œå®Œæˆ")
            print(f"      æˆåŠŸ: {results['completed_tasks']}, å¤±è´¥: {results['failed_tasks']}")
            if 'tasks_results' in results and results['tasks_results']:
                first_result = results['tasks_results'][0]
                print(f"      é¦–ä¸ªç»“æœé¢„è§ˆ: {first_result.get('response', '')[:50]}...")

        executor.shutdown()
    except Exception as e:
        print(f"   âŒ æµ‹è¯•æ‰§è¡Œå™¨å¤±è´¥: {e}")
        return False
    
    # 4. æµ‹è¯•AIè£åˆ¤æ¨¡å—
    print("\n4ï¸âƒ£ æµ‹è¯•AIè£åˆ¤æ¨¡å—...")
    try:
        ai_judge = AIJudgeClient()
        test_brand = brand_list[0]
        test_question = custom_questions[0].replace('{brandName}', test_brand)
        test_response = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“ç‰Œçš„ä»‹ç»ï¼Œæä¾›å¤šç§äº§å“å’ŒæœåŠ¡ã€‚"
        
        judge_result = ai_judge.evaluate_response(test_brand, test_question, test_response)
        
        if judge_result:
            print(f"   âœ… è£åˆ¤è¯„ä¼°æˆåŠŸ")
            print(f"      æƒå¨åº¦: {judge_result.accuracy_score}")
            print(f"      å¯è§åº¦: {judge_result.completeness_score}")
            print(f"      å¥½æ„Ÿåº¦: {judge_result.sentiment_score}")
            print(f"      çº¯å‡€åº¦: {judge_result.purity_score}")
            print(f"      ä¸€è‡´æ€§: {judge_result.consistency_score}")
            print(f"      è¯„ä»·: {judge_result.judgement[:50]}...")
        else:
            print(f"   âš ï¸  è£åˆ¤è¯„ä¼°è¿”å›None (å¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰é…ç½®è£åˆ¤API)")
    except Exception as e:
        print(f"   âš ï¸  AIè£åˆ¤æ¨¡å—æµ‹è¯•å¤±è´¥ (è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„): {e}")
    
    # 5. æµ‹è¯•è¯„åˆ†å¼•æ“
    print("\n5ï¸âƒ£ æµ‹è¯•è¯„åˆ†å¼•æ“...")
    try:
        scoring_engine = ScoringEngine()
        enhanced_scoring_engine = EnhancedScoringEngine()
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„è£åˆ¤ç»“æœç”¨äºæµ‹è¯•
        from ai_judge_module import JudgeResult, ConfidenceLevel
        mock_judge_results = [
            JudgeResult(
                accuracy_score=85,
                completeness_score=78,
                sentiment_score=82,
                purity_score=75,
                consistency_score=80,
                judgement="å›ç­”è¾ƒä¸ºå‡†ç¡®å®Œæ•´",
                confidence_level=ConfidenceLevel.HIGH
            ),
            JudgeResult(
                accuracy_score=90,
                completeness_score=85,
                sentiment_score=75,
                purity_score=80,
                consistency_score=88,
                judgement="é«˜è´¨é‡å›ç­”",
                confidence_level=ConfidenceLevel.HIGH
            )
        ]
        
        # åŸºç¡€è¯„åˆ†
        basic_result = scoring_engine.calculate(mock_judge_results)
        print(f"   âœ… åŸºç¡€è¯„åˆ†å®Œæˆ")
        print(f"      GEOåˆ†æ•°: {basic_result.geo_score}")
        print(f"      ç­‰çº§: {basic_result.grade}")
        
        # å¢å¼ºè¯„åˆ†
        enhanced_result = calculate_enhanced_scores(mock_judge_results, brand_name=test_brand)
        print(f"   âœ… å¢å¼ºè¯„åˆ†å®Œæˆ")
        print(f"      å¢å¼ºGEOåˆ†æ•°: {enhanced_result.geo_score}")
        print(f"      è®¤çŸ¥ç½®ä¿¡åº¦: {enhanced_result.cognitive_confidence:.2f}")
        
    except Exception as e:
        print(f"   âŒ è¯„åˆ†å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print("\nâœ… å“ç‰Œè¯Šæ–­å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•æˆåŠŸ!")
    return True


def test_frontend_simulation():
    """æ¨¡æ‹Ÿå‰ç«¯è¯·æ±‚æµç¨‹"""
    print("\nğŸŒ æ¨¡æ‹Ÿå‰ç«¯è¯·æ±‚æµç¨‹...")
    
    # æ¨¡æ‹Ÿå‰ç«¯å‘é€çš„æ•°æ®
    frontend_data = {
        "brand_list": ["è”šæ¥æ±½è½¦", "ç†æƒ³æ±½è½¦"],
        "selectedModels": [
            {"name": "è±†åŒ…", "checked": True}
        ],
        "customQuestions": [
            "ä»‹ç»ä¸€ä¸‹{brandName}",
            "{brandName}çš„ä¸»è¦äº§å“æ˜¯ä»€ä¹ˆï¼Ÿ"
        ],
        "userOpenid": "test_user_openid",
        "userLevel": "Free"
    }
    
    print(f"ğŸ“¤ æ¨¡æ‹Ÿå‰ç«¯æ•°æ®:")
    print(f"   å“ç‰Œ: {frontend_data['brand_list']}")
    print(f"   é€‰æ‹©å¹³å°: {[m['name'] for m in frontend_data['selectedModels'] if m['checked']]}")
    print(f"   é—®é¢˜æ•°é‡: {len(frontend_data['customQuestions'])}")
    
    # æ¨¡æ‹Ÿåç«¯å¤„ç†æµç¨‹
    try:
        from wechat_backend.views import process_and_aggregate_results_with_ai_judge
        from wechat_backend.question_system import QuestionManager, TestCaseGenerator
        from wechat_backend.test_engine import TestExecutor, ExecutionStrategy
        
        # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        question_manager = QuestionManager()
        test_case_generator = TestCaseGenerator()
        
        brand_list = frontend_data['brand_list']
        selected_models = frontend_data['selectedModels']
        custom_questions = [q.strip() for q in frontend_data['customQuestions'] if q.strip()]
        
        # ä¸ºæ¯ä¸ªå“ç‰Œç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        all_test_cases = []
        for brand in brand_list:
            brand_questions = [q.replace('{brandName}', brand) for q in custom_questions]
            cases = test_case_generator.generate_test_cases(brand, selected_models, brand_questions)
            all_test_cases.extend(cases)
        
        print(f"ğŸ“ ç”Ÿæˆäº† {len(all_test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        # æ¨¡æ‹Ÿæ‰§è¡Œæµ‹è¯•ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿç»“æœï¼Œå› ä¸ºå®é™…æ‰§è¡Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨TestExecutoræ‰§è¡ŒçœŸå®çš„APIè°ƒç”¨
        mock_raw_results = []
        for case in all_test_cases:
            # æ¨¡æ‹ŸAPIå“åº”
            mock_response = f"è¿™æ˜¯å…³äº{case.brand_name}çš„{case.question}çš„å›ç­”ã€‚"
            mock_raw_results.append({
                'success': True,
                'brand_name': case.brand_name,
                'model': case.ai_model,  # ä½¿ç”¨æ­£ç¡®çš„å±æ€§å
                'question': case.question,
                'result': {'content': mock_response},
                'response': mock_response
            })
        
        # å¤„ç†å’Œèšåˆç»“æœ
        results = process_and_aggregate_results_with_ai_judge(
            mock_raw_results, 
            brand_list, 
            brand_list[0]  # ä¸»å“ç‰Œ
        )
        
        print(f"ğŸ“Š å¤„ç†ç»“æœ:")
        print(f"   è¯¦ç»†ç»“æœæ•°é‡: {len(results['detailed_results'])}")
        print(f"   ä¸»å“ç‰Œæ•°æ®: {results['main_brand']['overallScore']} åˆ†")
        print(f"   ç«å“åˆ†æ: {len(results['competitiveAnalysis']['brandScores'])} ä¸ªå“ç‰Œ")
        
        # æ˜¾ç¤ºä¸€äº›ç»“æœ
        if results['detailed_results']:
            first_result = results['detailed_results'][0]
            print(f"   é¦–ä¸ªç»“æœé¢„è§ˆ: {first_result['brand']} - {first_result['aiModel']}")
        
        print("âœ… å‰ç«¯æ¨¡æ‹Ÿæµç¨‹æµ‹è¯•æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âŒ å‰ç«¯æ¨¡æ‹Ÿæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è±†åŒ…APIä¸å“ç‰Œè¯Šæ–­åŠŸèƒ½é›†æˆæµ‹è¯•")
    print("="*60)
    
    # è¿è¡Œæµ‹è¯•
    test1_success = test_brand_diagnosis_workflow()
    test2_success = test_frontend_simulation()
    
    print("\n" + "="*60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
    print(f"   å“ç‰Œè¯Šæ–­å·¥ä½œæµç¨‹: {'âœ… é€šè¿‡' if test1_success else 'âŒ å¤±è´¥'}")
    print(f"   å‰ç«¯æ¨¡æ‹Ÿæµç¨‹: {'âœ… é€šè¿‡' if test2_success else 'âŒ å¤±è´¥'}")
    
    overall_success = test1_success and test2_success
    print(f"\n   æ€»ä½“ç»“æœ: {'âœ… æˆåŠŸ' if overall_success else 'âŒ å¤±è´¥'}")
    
    if overall_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è±†åŒ…APIä¸å“ç‰Œè¯Šæ–­åŠŸèƒ½é›†æˆæ­£å¸¸å·¥ä½œã€‚")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³æ¨¡å—ã€‚")
    
    return overall_success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)