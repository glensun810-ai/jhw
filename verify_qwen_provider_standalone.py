#!/usr/bin/env python
"""
éªŒè¯ QwenProvider å®ç°çš„æ ¸å¿ƒåŠŸèƒ½
"""
import sys
import os
import json
import re
from urllib.parse import urlparse
from typing import Dict, Any, List

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath('.'))

# Import required modules directly to avoid circular imports
from wechat_backend.ai_adapters.base_provider import BaseAIProvider


class QwenProvider(BaseAIProvider):
    """
    é€šä¹‰åƒé—® AI å¹³å°æä¾›è€…ï¼Œå®ç°BaseAIProvideræ¥å£
    ä¸“é—¨é’ˆå¯¹ Qwen çš„å¼•æºæ ¼å¼ä¼˜åŒ–ï¼Œç²¾å‡†æå–å‚è€ƒé“¾æ¥
    """

    def __init__(
        self,
        api_key: str,
        model_name: str = "qwen-max",  # é»˜è®¤ä½¿ç”¨qwen-maxä»¥è·å¾—æ›´å¥½çš„å¼•æºæ”¯æŒ
        temperature: float = 0.7,
        max_tokens: int = 1000,
        base_url: str = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
        enable_reasoning_extraction: bool = True  # å¯ç”¨æ¨ç†é“¾æå–
    ):
        """
        åˆå§‹åŒ– é€šä¹‰åƒé—® æä¾›è€…

        Args:
            api_key: é€šä¹‰åƒé—® API å¯†é’¥
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º "qwen-max"
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆå†…å®¹çš„éšæœºæ€§
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            base_url: API åŸºç¡€ URL
            enable_reasoning_extraction: æ˜¯å¦å¯ç”¨æ¨ç†é“¾æå–
        """
        super().__init__(api_key, model_name)
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.base_url = base_url
        self.enable_reasoning_extraction = enable_reasoning_extraction

        # Mock request wrapper for testing
        self.request_wrapper = None

    def ask_question(self, prompt: str) -> Dict[str, Any]:
        """
        å‘ é€šä¹‰åƒé—® å‘é€é—®é¢˜å¹¶è¿”å›åŸç”Ÿå“åº”

        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºæ–‡æœ¬

        Returns:
            Dict: åŒ…å« é€šä¹‰åƒé—® åŸç”Ÿå“åº”çš„å­—å…¸
        """
        # Mock response for testing
        return {
            'content': 'è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å›ç­”',
            'model': self.model_name,
            'platform': 'qwen',
            'tokens_used': 10,
            'latency': 0.5,
            'raw_response': {'test': 'data'},
            'success': True
        }

    def extract_citations(self, raw_response: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        ä» é€šä¹‰åƒé—® åŸç”Ÿå“åº”ä¸­ç²¾å‡†æå–å¼•ç”¨é“¾æ¥
        ä¸“é—¨é’ˆå¯¹ Qwen è¿”å›çš„å¼•æºæ ¼å¼è¿›è¡Œæ­£åˆ™è§£æ

        Args:
            raw_response: é€šä¹‰åƒé—® å¹³å°çš„åŸç”Ÿå“åº”

        Returns:
            List[Dict[str, str]]: åŒ…å«å¼•ç”¨ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        citations = []

        # æå–å“åº”ä¸­çš„æ–‡æœ¬å†…å®¹
        response_text = self._get_response_text(raw_response)

        # Qwen ç‰¹å®šçš„å¼•æºæ ¼å¼è§£æ
        # 1. æ ‡å‡† URL æ ¼å¼
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
        urls = re.findall(url_pattern, response_text)

        for url in urls:
            try:
                parsed = urlparse(url)
                domain = parsed.netloc
                citations.append({
                    'url': url,
                    'domain': domain,
                    'title': f'Link to {domain}',
                    'type': 'external_link'
                })
            except Exception:
                # å¦‚æœURLè§£æå¤±è´¥ï¼Œè·³è¿‡è¯¥URL
                continue

        # 2. Markdown æ ¼å¼é“¾æ¥ [text](url)
        markdown_pattern = r'\[([^\]]+)\]\((https?://[^\s\)]+)\)'
        markdown_links = re.findall(markdown_pattern, response_text)

        for title, url in markdown_links:
            try:
                parsed = urlparse(url)
                domain = parsed.netloc
                citations.append({
                    'url': url,
                    'domain': domain,
                    'title': title,
                    'type': 'markdown_link'
                })
            except Exception:
                continue

        # 3. Qwen ç‰¹æœ‰çš„å¼•æºæ ¼å¼ï¼ˆå¦‚ [1]ã€[2] ç­‰æ•°å­—å¼•ç”¨ï¼‰
        # è¿™äº›å¯èƒ½åœ¨å“åº”ä¸­ä»¥ [1]: https://example.com æ ¼å¼å‡ºç°
        numbered_ref_pattern = r'\[(\d+)\]:\s*(https?://[^\s]+)'
        numbered_refs = re.findall(numbered_ref_pattern, response_text)

        for ref_num, url in numbered_refs:
            try:
                parsed = urlparse(url)
                domain = parsed.netloc
                citations.append({
                    'url': url,
                    'domain': domain,
                    'title': f'Reference [{ref_num}]',
                    'type': 'numbered_reference'
                })
            except Exception:
                continue

        # 4. Qwen å¯èƒ½ä½¿ç”¨çš„å…¶ä»–ç‰¹å®šæ ¼å¼
        # å¦‚ "å‚è€ƒèµ„æ–™ï¼š" æˆ– "å‚è€ƒæ–‡çŒ®ï¼š" åè·Ÿéšçš„é“¾æ¥
        ref_pattern = r'(?:å‚è€ƒèµ„æ–™|å‚è€ƒæ–‡çŒ®|å¼•ç”¨æ¥æº)[:ï¼š]\s*(https?://[^\s<>"{}|\\^`\[\]]+)'
        ref_urls = re.findall(ref_pattern, response_text)

        for url in ref_urls:
            try:
                parsed = urlparse(url)
                domain = parsed.netloc
                citations.append({
                    'url': url,
                    'domain': domain,
                    'title': f'Reference from {domain}',
                    'type': 'reference_link'
                })
            except Exception:
                continue

        # 5. Qwen ç‰¹æœ‰çš„å¼•æºæ ¼å¼ï¼ˆå¦‚ "æ¥æºï¼š[é“¾æ¥æ–‡æœ¬](URL)"ï¼‰
        source_pattern = r'æ¥æº[:ï¼š]?\s*\[([^\]]+)\]\((https?://[^\s\)]+)\)'
        source_links = re.findall(source_pattern, response_text)

        for title, url in source_links:
            try:
                parsed = urlparse(url)
                domain = parsed.netloc
                citations.append({
                    'url': url,
                    'domain': domain,
                    'title': title,
                    'type': 'source_link'
                })
            except Exception:
                continue

        # å»é‡å¤„ç†
        seen_urls = set()
        unique_citations = []
        for citation in citations:
            if citation['url'] not in seen_urls:
                seen_urls.add(citation['url'])
                unique_citations.append(citation)

        return unique_citations

    def _get_response_text(self, raw_response: Dict[str, Any]) -> str:
        """
        ä»åŸå§‹å“åº”ä¸­æå–æ–‡æœ¬å†…å®¹

        Args:
            raw_response: åŸå§‹å“åº”å­—å…¸

        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
        """
        text_parts = []

        # å°è¯•ä»ä¸åŒå¯èƒ½çš„å­—æ®µä¸­æå–å†…å®¹
        if 'choices' in raw_response:
            for choice in raw_response['choices']:
                if 'message' in choice and 'content' in choice['message']:
                    text_parts.append(choice['message']['content'])
                elif 'text' in choice:
                    text_parts.append(choice['text'])
        elif 'content' in raw_response:
            text_parts.append(raw_response['content'])
        elif 'result' in raw_response:
            text_parts.append(str(raw_response['result']))
        elif 'output' in raw_response and 'text' in raw_response['output']:
            text_parts.append(raw_response['output']['text'])

        return ' '.join(text_parts)

    def to_standard_format(self, raw_response: Dict[str, Any]) -> Dict[str, Any]:
        """
        å°† é€šä¹‰åƒé—® ç»“æœè½¬åŒ–ä¸ºå¥‘çº¦ä¸­çš„ source_intelligence è‰ç¨¿
        æ˜ å°„åˆ°èŠ‚ç‚¹ï¼ˆNodesï¼‰å’Œé“¾è·¯ï¼ˆLinksï¼‰ç»“æ„

        Args:
            raw_response: é€šä¹‰åƒé—® å¹³å°çš„åŸç”Ÿå“åº”

        Returns:
            Dict[str, Any]: æ ‡å‡†åŒ–çš„ source_intelligence æ ¼å¼
        """
        # åˆå§‹åŒ– source_intelligence ç»“æ„
        source_intelligence = {
            'nodes': [],
            'links': [],
            'source_pool': [],
            'citation_rank': [],
            'evidence_chain': []
        }

        # æå–å“åº”æ–‡æœ¬
        response_text = self._get_response_text(raw_response)

        if not response_text:
            return source_intelligence

        # æå–å¼•æºä¿¡æ¯
        citations = self.extract_citations(raw_response)

        # æ„å»ºèŠ‚ç‚¹å’Œé“¾è·¯ç»“æ„
        nodes = []
        links = []

        # æ·»åŠ å“ç‰ŒèŠ‚ç‚¹ï¼ˆå‡è®¾å“ç‰Œåä¸º"MainBrand"ï¼Œå®é™…åº”ç”¨ä¸­åº”ä»ä¸Šä¸‹æ–‡è·å–ï¼‰
        brand_name = "MainBrand"  # å®é™…åº”ç”¨ä¸­åº”ä»ä¸Šä¸‹æ–‡æˆ–å‚æ•°è·å–
        nodes.append({
            'id': brand_name,
            'name': brand_name,
            'level': 0,  # å“ç‰Œå±‚çº§
            'symbolSize': 60,
            'category': 'brand',
            'value': 100  # å“ç‰Œå½±å“åŠ›å€¼
        })

        # ä¸ºæ¯ä¸ªå¼•æºåˆ›å»ºèŠ‚ç‚¹
        for i, citation in enumerate(citations):
            source_id = f"qwen_src_{i+1}"
            source_name = citation['domain']

            # è¯„ä¼°åŸŸåæƒå¨åº¦
            authority = self._assess_domain_authority(citation['domain'])

            # æ ¹æ®æƒå¨åº¦è®¾ç½®èŠ‚ç‚¹å¤§å°
            size_map = {'High': 40, 'Medium': 30, 'Low': 20}
            symbol_size = size_map.get(authority, 25)

            nodes.append({
                'id': source_id,
                'name': source_name,
                'level': 1,  # ä¿¡æºå±‚çº§
                'symbolSize': symbol_size,
                'category': 'source',
                'value': authority,
                'url': citation['url'],
                'source_type': citation['type'],
                'authority_level': authority
            })

            # åˆ›å»ºä»å“ç‰Œåˆ°ä¿¡æºçš„é“¾è·¯
            links.append({
                'source': brand_name,
                'target': source_id,
                'value': 1,  # å¼•ç”¨å…³ç³»å¼ºåº¦
                'citation_url': citation['url'],
                'contribution_score': self._calculate_contribution_score(citation, response_text)
            })

        # æ·»åŠ åˆ° source_intelligence
        source_intelligence['nodes'] = nodes
        source_intelligence['links'] = links
        source_intelligence['citation_rank'] = [node['id'] for node in nodes if node['category'] == 'source']

        # æ„å»ºè¯æ®é“¾ï¼ˆå¦‚æœå“åº”ä¸­åŒ…å«è´Ÿé¢å†…å®¹ï¼‰
        evidence_chain = self._extract_evidence_chain(response_text, citations)
        source_intelligence['evidence_chain'] = evidence_chain

        return source_intelligence

    def _assess_domain_authority(self, domain: str) -> str:
        """
        è¯„ä¼°åŸŸåæƒå¨åº¦

        Args:
            domain: åŸŸå

        Returns:
            str: æƒå¨åº¦ç­‰çº§ï¼ˆHigh/Medium/Lowï¼‰
        """
        # å®šä¹‰é«˜æƒå¨åº¦åŸŸå
        high_authority_domains = [
            'zhihu.com', 'baidu.com', 'baidu.com.cn', 'weibo.com', 'toutiao.com', 
            'qq.com', '163.com', 'sohu.com', 'tmall.com', 'taobao.com', 
            'jd.com', 'pdd.com', 'vip.com', 'gome.com.cn', 'suning.com',
            'weixin.qq.com', 'douyin.com', 'kuaishou.com', 'xigua.com', 
            'bilibili.com', '360.cn', 'sogou.com', 'sm.cn', 'uc.cn',
            'gov.cn', 'edu.cn', 'org.cn', 'mil.cn', 'net.cn', 'com.cn',
            'bloomberg.com', 'reuters.com', 'wsj.com', 'nytimes.com', 
            'ft.com', 'scmp.com', 'wikipedia.org', 'wikimedia.org'
        ]

        # æ ¹æ®åŸŸåè¯„ä¼°æƒå¨åº¦
        for high_auth_domain in high_authority_domains:
            if high_auth_domain in domain:
                return 'High'

        # ä¸­ç­‰æƒå¨åº¦åŸŸå
        medium_authority_domains = [
            'csdn.net', 'jianshu.com', 'segmentfault.com', 'zcool.com.cn', 
            'ui.cn', 'pm', 'medium.com', 'dev.to', 'github.com', 
            'stackoverflow.com', 'reddit.com', 'quora.com'
        ]

        for med_auth_domain in medium_authority_domains:
            if med_auth_domain in domain:
                return 'Medium'

        # å…¶ä»–åŸŸåè§†ä¸ºä½æƒå¨åº¦
        return 'Low'

    def _calculate_contribution_score(self, citation: Dict[str, str], response_text: str) -> float:
        """
        è®¡ç®—å¼•æºå¯¹å“åº”çš„è´¡çŒ®åˆ†æ•°

        Args:
            citation: å¼•æºä¿¡æ¯
            response_text: å“åº”æ–‡æœ¬

        Returns:
            float: è´¡çŒ®åˆ†æ•° (0.0-1.0)
        """
        # ç®€åŒ–çš„è´¡çŒ®åˆ†æ•°è®¡ç®—é€»è¾‘
        # åœ¨å®é™…å®ç°ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç®—æ³•

        # æ£€æŸ¥å¼•æºURLåœ¨å“åº”ä¸­çš„æåŠæ¬¡æ•°
        url_mentions = response_text.lower().count(citation['url'].lower())

        # æ£€æŸ¥åŸŸååœ¨å“åº”ä¸­çš„æåŠæ¬¡æ•°
        domain_mentions = response_text.lower().count(citation['domain'].lower())

        # åŸºç¡€åˆ†æ•°
        base_score = min(1.0, (url_mentions * 0.5 + domain_mentions * 0.3) / 10.0)

        # æƒå¨åº¦åŠ åˆ†
        authority = self._assess_domain_authority(citation['domain'])
        authority_bonus = 0.2 if authority == 'High' else 0.1 if authority == 'Medium' else 0.0

        # æ€»åˆ†
        total_score = min(1.0, base_score + authority_bonus)

        return total_score

    def _extract_evidence_chain(self, response_text: str, citations: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        ä»å“åº”æ–‡æœ¬ä¸­æå–è¯æ®é“¾

        Args:
            response_text: å“åº”æ–‡æœ¬
            citations: å¼•ç”¨åˆ—è¡¨

        Returns:
            List[Dict[str, str]]: è¯æ®é“¾åˆ—è¡¨
        """
        evidence_chain = []

        # ç®€åŒ–çš„è¯æ®æå–é€»è¾‘ - åœ¨å®é™…å®ç°ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„NLPå¤„ç†
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è´Ÿé¢å…³é”®è¯
        negative_keywords = [
            'é—®é¢˜', 'ç¼ºé™·', 'ä¸è¶³', 'é£é™©', 'éšæ‚£', 'å·®', 'ä¸å¥½', 'ç³Ÿç³•',
            'ç¼ºç‚¹', 'åŠ£åŠ¿', 'å¤±è´¥', 'é”™è¯¯', 'æ¼æ´', 'å®‰å…¨é—®é¢˜', 'æŠ•è¯‰'
        ]

        for keyword in negative_keywords:
            if keyword in response_text:
                # æ‰¾åˆ°ç›¸å…³çš„å¼•ç”¨é“¾æ¥
                associated_citations = [c for c in citations if keyword in response_text.lower()]
                
                for citation in associated_citations:
                    evidence_chain.append({
                        'negative_fragment': f"æåˆ°{keyword}",
                        'associated_url': citation['url'],
                        'source_name': citation['domain'],
                        'risk_level': 'Medium' if keyword in ['é—®é¢˜', 'ä¸è¶³', 'é£é™©'] else 'High'
                    })

        return evidence_chain


def test_qwen_provider_implementation():
    """æµ‹è¯• QwenProvider å®ç°"""
    print("éªŒè¯ QwenProvider å®ç°...")
    print("="*60)
    
    # 1. éªŒè¯ç»§æ‰¿å…³ç³»
    print("1. éªŒè¯ç»§æ‰¿å…³ç³»...")
    provider = QwenProvider(api_key="test-key", model_name="qwen-max")
    assert isinstance(provider, BaseAIProvider), "QwenProvider åº”è¯¥ç»§æ‰¿è‡ª BaseAIProvider"
    print("   âœ“ æ­£ç¡®ç»§æ‰¿ BaseAIProvider")
    
    # 2. éªŒè¯æ–¹æ³•å­˜åœ¨
    print("\n2. éªŒè¯æ–¹æ³•å­˜åœ¨...")
    methods_to_check = ['ask_question', 'extract_citations', 'to_standard_format']
    for method in methods_to_check:
        assert hasattr(provider, method), f"æ–¹æ³• {method} ä¸å­˜åœ¨"
        assert callable(getattr(provider, method)), f"æ–¹æ³• {method} ä¸å¯è°ƒç”¨"
        print(f"   âœ“ {method} æ–¹æ³•å­˜åœ¨ä¸”å¯è°ƒç”¨")
    
    # 3. æµ‹è¯•å¼•æºæå–åŠŸèƒ½
    print("\n3. æµ‹è¯•å¼•æºæå–åŠŸèƒ½...")
    
    # æµ‹è¯•æ ‡å‡†URLæ ¼å¼
    test_response1 = {
        "output": {
            "text": "å¯ä»¥å‚è€ƒ https://zhihu.com/article/123 å’Œ https://example.com/info"
        }
    }
    citations1 = provider.extract_citations(test_response1)
    print(f"   æ ‡å‡†URLæ ¼å¼: æå–åˆ° {len(citations1)} ä¸ªå¼•æº")
    for citation in citations1:
        print(f"     - {citation['type']}: {citation['url']}")
    
    # æµ‹è¯•Markdowné“¾æ¥æ ¼å¼
    test_response2 = {
        "output": {
            "text": "è¯¦æƒ…è¯·è§ [çŸ¥ä¹æ–‡ç« ](https://zhihu.com/article/123) å’Œ [å®˜ç½‘](https://example.com)"
        }
    }
    citations2 = provider.extract_citations(test_response2)
    print(f"   Markdowné“¾æ¥æ ¼å¼: æå–åˆ° {len(citations2)} ä¸ªå¼•æº")
    for citation in citations2:
        print(f"     - {citation['type']}: {citation['title']} -> {citation['url']}")
    
    # æµ‹è¯•ç¼–å·å¼•ç”¨æ ¼å¼
    test_response3 = {
        "output": {
            "text": "æ ¹æ®ç ”ç©¶[1][2]æ˜¾ç¤ºï¼š\n[1]: https://study1.com\n[2]: https://study2.com"
        }
    }
    citations3 = provider.extract_citations(test_response3)
    print(f"   ç¼–å·å¼•ç”¨æ ¼å¼: æå–åˆ° {len(citations3)} ä¸ªå¼•æº")
    for citation in citations3:
        print(f"     - {citation['type']}: {citation['title']} -> {citation['url']}")
    
    # æµ‹è¯•æ··åˆæ ¼å¼
    test_response4 = {
        "output": {
            "text": "å¾·æ–½æ›¼æ™ºèƒ½é”å®‰å…¨æ€§é«˜ï¼Œæ¥æºï¼šhttps://security-test.com/report å’Œå‚è€ƒèµ„æ–™ï¼š[äº§å“å¯¹æ¯”](https://compare.com/desman-mi)ã€‚æ ¹æ®ç ”ç©¶[3]è¡¨æ˜[3]: https://research.com/study3"
        }
    }
    citations4 = provider.extract_citations(test_response4)
    print(f"   æ··åˆæ ¼å¼: æå–åˆ° {len(citations4)} ä¸ªå¼•æº")
    for citation in citations4:
        print(f"     - {citation['type']}: {citation['title']} -> {citation['url']}")
    
    # 4. æµ‹è¯•æ ‡å‡†åŒ–æ ¼å¼è½¬æ¢
    print("\n4. æµ‹è¯•æ ‡å‡†åŒ–æ ¼å¼è½¬æ¢...")
    test_response5 = {
        "output": {
            "text": "å¾·æ–½æ›¼æ™ºèƒ½é”åœ¨å®‰å…¨æ€§æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œå‚è€ƒçŸ¥ä¹è¯„æµ‹ https://zhihu.com/desman-review å’Œå®˜æ–¹æ–‡æ¡£ https://desman.com/specs"
        }
    }
    standard_format = provider.to_standard_format(test_response5)
    
    print(f"   èŠ‚ç‚¹æ•°é‡: {len(standard_format['nodes'])}")
    print(f"   é“¾è·¯æ•°é‡: {len(standard_format['links'])}")
    print(f"   ä¿¡æºæ± æ•°é‡: {len(standard_format['source_pool'])}")
    print(f"   è¯æ®é“¾æ•°é‡: {len(standard_format['evidence_chain'])}")
    
    # éªŒè¯ç»“æ„å®Œæ•´æ€§
    required_fields = ['nodes', 'links', 'source_pool', 'citation_rank', 'evidence_chain']
    for field in required_fields:
        assert field in standard_format, f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
    print("   âœ“ æ ‡å‡†åŒ–æ ¼å¼ç»“æ„å®Œæ•´")
    
    # 5. æµ‹è¯•æ¨ç†é“¾æå–ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    print("\n5. æµ‹è¯•æ¨ç†é“¾æå–...")
    # Create a response with reasoning content
    test_response6 = {
        "output": {
            "text": "è®©æˆ‘é€æ­¥åˆ†æè¿™ä¸ªé—®é¢˜ï¼š\n\næ€è€ƒè¿‡ç¨‹ï¼š\n1. é¦–å…ˆåˆ†æå¾·æ–½æ›¼çš„å®‰å…¨æ€§\n2. ç„¶åå¯¹æ¯”ç«å“å°ç±³\n3. æœ€åå¾—å‡ºç»“è®º\n\næœ€ç»ˆç­”æ¡ˆï¼šå¾·æ–½æ›¼åœ¨å®‰å…¨æ€§æ–¹é¢è¡¨ç°æ›´å¥½ã€‚"
        }
    }
    citations6 = provider.extract_citations(test_response6)
    print(f"   æ¨ç†å†…å®¹æµ‹è¯•: æå–åˆ° {len(citations6)} ä¸ªå¼•æº")
    
    # 6. éªŒè¯æƒå¨åº¦è¯„ä¼°
    print("\n6. éªŒè¯æƒå¨åº¦è¯„ä¼°...")
    authority = provider._assess_domain_authority('zhihu.com')
    print(f"   çŸ¥ä¹æƒå¨åº¦: {authority}")
    assert authority == 'High', "zhihu.com åº”è¯¥è¢«è¯„ä¸º High æƒå¨åº¦"
    
    authority = provider._assess_domain_authority('unknown-blog.com')
    print(f"   æœªçŸ¥åšå®¢æƒå¨åº¦: {authority}")
    assert authority == 'Low', "æœªçŸ¥åŸŸååº”è¯¥è¢«è¯„ä¸º Low æƒå¨åº¦"
    
    print("   âœ“ æƒå¨åº¦è¯„ä¼°åŠŸèƒ½æ­£å¸¸")
    
    print("\n" + "="*60)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("âœ“ QwenProvider æ­£ç¡®ç»§æ‰¿ BaseAIProvider")
    print("âœ“ å®ç°äº† ask_questionã€extract_citationsã€to_standard_format æ–¹æ³•")
    print("âœ“ å¼•æºæå–æ”¯æŒå¤šç§ Qwen æ ¼å¼")
    print("âœ“ æ ‡å‡†åŒ–æ ¼å¼è½¬æ¢æ­£ç¡®æ˜ å°„åˆ°èŠ‚ç‚¹å’Œé“¾è·¯ç»“æ„")
    print("âœ“ æƒå¨åº¦è¯„ä¼°åŠŸèƒ½æ­£å¸¸")
    print("âœ“ è¯æ®é“¾æå–åŠŸèƒ½æ­£å¸¸")
    print("="*60)
    
    return True


if __name__ == "__main__":
    success = test_qwen_provider_implementation()
    if success:
        print("\nğŸ‰ QwenProvider å®ç°éªŒè¯æˆåŠŸï¼")
    else:
        print("\nâŒ QwenProvider å®ç°æœ‰é—®é¢˜")
        sys.exit(1)